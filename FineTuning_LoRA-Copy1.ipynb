{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0da33-493a-4314-93a4-cdf1355f1adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cce9f6b6-f2b2-407e-85fa-ced2d1df64b6",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9510b818-0e27-412e-b11a-820c6d38227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 80870\n",
      "Test dataset size: 10110\n",
      "Valid dataset size: 10108\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_name = \"train.json\"\n",
    "valid_name = \"valid.json\"\n",
    "test_name = \"test.json\"\n",
    "data_dir = \"./Data\"\n",
    "\n",
    "data_files = {\"train\": train_name, \"test\": test_name, \"valid\": valid_name}\n",
    "dataset = load_dataset('json', data_dir = data_dir, data_files = data_files)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "print(f\"Valid dataset size: {len(dataset['valid'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62182466-67c8-4837-b481-e7a7376dc08b",
   "metadata": {},
   "source": [
    "# Preparation before training\n",
    "\n",
    "Before the training of LLM, we need to do preliminary-disposition of dataset.\n",
    "GQA belongs to the Text-Generation task.\n",
    "We need to know the length information of the input & output text, which will benefit for the high-efficient batch-processing for these dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef009b98-8c26-4055-ab48-9495f260a813",
   "metadata": {},
   "source": [
    "* We utilize the t5-large model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b925582d-8741-44c1-98a5-3ca437bba26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_path = \"/root/autodl-fs/flan-t5-xxl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1adad5c1-4d19-4244-b3cf-cafb4d354be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX source length: 34\n",
      "MAX target length: 90\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than the max will be truncated, and sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"question\"], truncation=True), batched = True, remove_columns=[\"question\", \"answer\"])\n",
    "input_lengths = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lengths, 85))\n",
    "print(f\"MAX source length: {max_source_length}\")\n",
    "\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"answer\"], truncation=True), batched=True, remove_columns=[\"question\", \"answer\"])\n",
    "target_lengths = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lengths, 90))\n",
    "print(f\"MAX target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f267c-2243-4cfe-8136-4c3d18ce8752",
   "metadata": {},
   "source": [
    "## We do pre-processing for all dataset and save the processed dataset to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf8a38d4-b786-4b99-9ada-fc62bd3a5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(samples, padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [item for item in samples[\"question\"]]\n",
    "    \n",
    "    # tokenize the inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "    \n",
    "    # tokenize targets with the 'text_target' keyword argument\n",
    "    labels = tokenizer(text_target=samples[\"answer\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "    \n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "        \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f55b926-593a-49c5-9f9d-01f03eed9bc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <font color=red>(Save to Disk)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d1bd88-9027-4287-b622-886ba9d43fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d1b44137884aab83b1c1256f7503c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9207aacaa1a41aab014a7bb4836bd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/80870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f28d97f5efb40ecbbf1d5030aa6eddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns = [\"question\", \"answer\", \"question_type\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"Data/Train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"Data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84a5a5-b220-4c4c-9fe3-f6e944a0b47c",
   "metadata": {},
   "source": [
    "# <font color=red size=10>(Load Dataset from the Disk)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed18870-6714-423b-bc3f-9ae7cc7a0af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test', 'valid'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns = [\"question\", \"answer\", \"question_type\", \"id\"])\n",
    "\n",
    "# 加载训练数据集\n",
    "tokenized_dataset[\"train\"] = load_from_disk(\"Data/Train\")\n",
    "\n",
    "# 加载测试数据集\n",
    "tokenized_dataset[\"test\"] = load_from_disk(\"Data/test\")\n",
    "\n",
    "# 打印加载的数据集以验证\n",
    "print(tokenized_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f09405-6538-41ed-8129-9ce00e6fe1e9",
   "metadata": {},
   "source": [
    "# LoRA & bnb-int8 to Fine-tuning the T5-large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6da0c664-1a0f-4f00-8911-9b8ae552610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"/root/autodl-fs/t5-large\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b72f23c7-13b7-4ef4-9c0f-0ba26103e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/other.py:147: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: trainable params: 2,949,120 || all params: 740,617,216 || trainable%: 0.39819760279512595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training,TaskType\n",
    "import wandb\n",
    "\n",
    "\n",
    "'''======================================'''\n",
    "config1 = {\"r\": 10,\n",
    "    \"lora_alpha\": 36,\n",
    "    \"target_modules\": [\"q\", \"v\"],\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"lora_only\",\n",
    "    \"task_type\": TaskType.SEQ_2_SEQ_LM\n",
    "         }\n",
    "'''======================================='''\n",
    "\n",
    "\n",
    "# define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r = config1[\"r\"],\n",
    "    lora_alpha = config1[\"lora_alpha\"],\n",
    "    target_modules = config1[\"target_modules\"],\n",
    "    lora_dropout = config1[\"lora_dropout\"],\n",
    "    bias = config1[\"bias\"],\n",
    "    task_type = config1[\"task_type\"]\n",
    ")\n",
    "\n",
    "\n",
    "# prepare int8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adapter\n",
    "model = get_peft_model(model, lora_config)\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# 假设 model 是您已经定义好的模型对象\n",
    "\n",
    "# 创建一个 StringIO 对象\n",
    "output = io.StringIO()\n",
    "\n",
    "# 保存当前的 stdout\n",
    "original_stdout = sys.stdout\n",
    "\n",
    "try:\n",
    "    # 将 stdout 重定向到 StringIO 对象\n",
    "    sys.stdout = output\n",
    "    # 调用方法，打印输出到 StringIO 对象\n",
    "    model.print_trainable_parameters()\n",
    "finally:\n",
    "    # 恢复原始的 stdout\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "# 获取 StringIO 对象中的内容\n",
    "output_str = output.getvalue()\n",
    "\n",
    "# 关闭 StringIO 对象\n",
    "output.close()\n",
    "\n",
    "# 打印捕获到的字符串内容\n",
    "print(\"1:\",output_str)\n",
    "\n",
    "# wandb.init(\n",
    "#     project = \"LLM_LoRA_FineTuning\",\n",
    "    \n",
    "#     config = {\n",
    "#         \"config\": config1,\n",
    "#         \"Dataset\": \"qa_Tools_and_Home_Improvement\",\n",
    "#         \"Tuning-method\": \"LoRA\",\n",
    "#         \"Trainable params\": str(output_str)\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61f560-63af-4899-9ceb-fefa7aca7c79",
   "metadata": {},
   "source": [
    "接下来需要创建一个 $DataCollator$，负责对输入和标签进行填充，我们使用 🤗 $Transformers$ 库中的$DataCollatorForSeq2Seq$ 来完成这一环节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0e27782-5555-499c-9737-717bd91c7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100 \n",
    "# 在训练序列到序列（Seq2Seq）模型时，通常会使用特殊的填充标记来对输入进行处理。对于标签数据，在计算损失函数时，我们需要忽略填充标记所带来的影响，因为这些填充部分不应该参与到损失的计算中。\n",
    "# 在 Transformers 库中，通常将不应该被考虑的标签设置为一个特定的值，通常是 -100。当计算损失函数时，模型会忽略这些 -100 值所对应的预测结果，只计算真实标签部分的损失值，从而实现在计算损失函数时忽略填充标记的效果。 \n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b3801-7432-4390-9a21-ffea1df2d857",
   "metadata": {},
   "source": [
    "自定义回调函数来记录训练损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4f5e639-628c-401c-9555-4503714befdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "class TrainLogCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if 'loss' in state.log_history[-1]:\n",
    "            self.train_losses.append(state.log_history[-1]['loss'])\n",
    "            if state.global_step % args.logging_steps == 0:\n",
    "                print(f\"Logging step {state.global_step} at epoch {state.epoch}\")\n",
    "                wandb.log({\"logging_step\": state.global_step, \"train_loss\": state.log_history[-1]['loss']})\n",
    "            # print(f\"Step: {state.global_step}, Train Loss: {state.log_history[-1]['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d53dd30f-8091-439a-8ec1-2e7eb65b0f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:tnnnrg0n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>logging_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▅▂▃▃▂▄█▂▁▁▃▃▄▄▃▃▄▃▅▅▅▅▄▄▅▅▆▇█▆▆▆▆▅▅█▇▇▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>logging_step</td><td>10400</td></tr><tr><td>train_loss</td><td>3.3955</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rural-monkey-16</strong> at: <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/tnnnrg0n' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/tnnnrg0n</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240613_165335-tnnnrg0n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:tnnnrg0n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39878d1801a42d98deed94077a44ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112511116597388, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/Task-1/wandb/run-20240613_191636-p964vjht</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/p964vjht' target=\"_blank\">azure-cloud-17</a></strong> to <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/p964vjht' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/p964vjht</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "'''==============================================='''\n",
    "output_dir=\"t5-large3\"\n",
    "\n",
    "config2 = {\n",
    "    \"output_dir\": output_dir,\n",
    "    \"auto_find_batch_size\": True,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"num_train_epochs\":5,\n",
    "    \"logging_dir\":f\"{output_dir}/logs\",\n",
    "    \"logging_strategy\":\"steps\",\n",
    "    \"logging_steps\":200,\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"report_to\": \"tensorboard\"\n",
    "}\n",
    "'''================================================='''\n",
    "\n",
    "\n",
    "# 初始化自定义回调\n",
    "log_step_callback = TrainLogCallback()\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = config2[\"output_dir\"],\n",
    "    auto_find_batch_size = config2[\"auto_find_batch_size\"],\n",
    "    learning_rate = config2[\"learning_rate\"], # higher learning rate\n",
    "    num_train_epochs = config2[\"num_train_epochs\"],\n",
    "    logging_dir = config2[\"logging_dir\"],\n",
    "    logging_strategy = config2[\"logging_strategy\"],\n",
    "    logging_steps = config2[\"logging_steps\"],\n",
    "    save_strategy = config2[\"save_strategy\"],\n",
    "    report_to = config2[\"report_to\"],\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project = \"LLM_LoRA_FineTuning\",\n",
    "    \n",
    "    config = {\n",
    "        \"config1\": config1,\n",
    "        \"config2\": config2,\n",
    "        \"Dataset\": \"qa_Tools_and_Home_Improvement\",\n",
    "        \"Tuning-method\": \"LoRA\",\n",
    "        \"Trainable params\": str(output_str)\n",
    "    }\n",
    ")\n",
    "\n",
    "# api = wandb.Api()\n",
    "\n",
    "# run = api.run(\"llm_learner/LLM_LoRA_FineTuning/623waqcp\")\n",
    "# run.config[\"config2\"] = config2\n",
    "# run.update()\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    callbacks=[log_step_callback]\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b2a41-28e9-440d-9fac-a54d9d143492",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36054041-e460-48b7-92c8-8ea17bdea7dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50545' max='50545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50545/50545 11:06:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.230200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.179300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.143600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.143600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>3.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>3.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>3.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>3.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>3.174100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>3.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>3.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>3.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>3.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>3.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>3.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>3.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>3.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>3.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>3.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>3.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>3.069900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>3.104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>3.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>3.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>3.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>3.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>3.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>3.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>3.108500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>3.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>3.090900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>3.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>3.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>3.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>3.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>3.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>3.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>3.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>3.091600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.108800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>3.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>3.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>3.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>3.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>3.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>3.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>3.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>3.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>3.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>3.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>3.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>3.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>3.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>3.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>3.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>3.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>3.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>3.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>3.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>3.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>3.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>3.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>3.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>3.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>3.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>3.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>3.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>3.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>3.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>3.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>3.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>3.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>3.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>3.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>3.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>3.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>3.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>3.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>3.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>3.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>3.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>3.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>3.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>3.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>3.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>3.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>3.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>3.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>3.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>3.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>3.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>3.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>3.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>3.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>3.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>3.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>2.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>2.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>3.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>2.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>3.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>3.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>3.025500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.995400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>3.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>3.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>3.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>3.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>3.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>3.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>3.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>3.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>3.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>3.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>3.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>2.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>2.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>2.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>3.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>3.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>2.991700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>3.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>3.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>2.996900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>3.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>2.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>3.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>2.992900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>3.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>2.970500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>3.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39400</td>\n",
       "      <td>2.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>3.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39800</td>\n",
       "      <td>3.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>2.998700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40400</td>\n",
       "      <td>3.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40600</td>\n",
       "      <td>2.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>2.985800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>2.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41200</td>\n",
       "      <td>2.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>2.993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41600</td>\n",
       "      <td>2.984400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41800</td>\n",
       "      <td>3.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42200</td>\n",
       "      <td>2.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42400</td>\n",
       "      <td>2.961900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42600</td>\n",
       "      <td>2.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42800</td>\n",
       "      <td>2.986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>2.931800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43200</td>\n",
       "      <td>2.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43400</td>\n",
       "      <td>2.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43600</td>\n",
       "      <td>2.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43800</td>\n",
       "      <td>2.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>2.985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44200</td>\n",
       "      <td>2.976700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44400</td>\n",
       "      <td>2.993100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44600</td>\n",
       "      <td>2.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44800</td>\n",
       "      <td>2.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>2.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45200</td>\n",
       "      <td>2.987100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45400</td>\n",
       "      <td>2.965500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45600</td>\n",
       "      <td>2.984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45800</td>\n",
       "      <td>2.959300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>2.940200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46200</td>\n",
       "      <td>2.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46400</td>\n",
       "      <td>2.953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46600</td>\n",
       "      <td>2.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46800</td>\n",
       "      <td>3.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>2.976700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47200</td>\n",
       "      <td>2.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47400</td>\n",
       "      <td>2.996400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47600</td>\n",
       "      <td>2.988200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47800</td>\n",
       "      <td>2.973600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>2.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48200</td>\n",
       "      <td>2.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48400</td>\n",
       "      <td>2.972700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48600</td>\n",
       "      <td>2.987600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48800</td>\n",
       "      <td>2.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>2.961800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49200</td>\n",
       "      <td>2.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49400</td>\n",
       "      <td>2.954500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49600</td>\n",
       "      <td>2.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49800</td>\n",
       "      <td>2.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>2.991500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50200</td>\n",
       "      <td>2.989600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50400</td>\n",
       "      <td>2.956200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging step 200 at epoch 0.019784350578692253\n",
      "Logging step 400 at epoch 0.039568701157384506\n",
      "Logging step 600 at epoch 0.059353051736076766\n",
      "Logging step 800 at epoch 0.07913740231476901\n",
      "Logging step 1000 at epoch 0.09892175289346128\n",
      "Logging step 1200 at epoch 0.11870610347215353\n",
      "Logging step 1400 at epoch 0.13849045405084578\n",
      "Logging step 1600 at epoch 0.15827480462953802\n",
      "Logging step 1800 at epoch 0.1780591552082303\n",
      "Logging step 2000 at epoch 0.19784350578692256\n",
      "Logging step 2200 at epoch 0.2176278563656148\n",
      "Logging step 2400 at epoch 0.23741220694430706\n",
      "Logging step 2600 at epoch 0.2571965575229993\n",
      "Logging step 2800 at epoch 0.27698090810169157\n",
      "Logging step 3000 at epoch 0.29676525868038384\n",
      "Logging step 3200 at epoch 0.31654960925907605\n",
      "Logging step 3400 at epoch 0.3363339598377683\n",
      "Logging step 3600 at epoch 0.3561183104164606\n",
      "Logging step 3800 at epoch 0.37590266099515285\n",
      "Logging step 4000 at epoch 0.3956870115738451\n",
      "Logging step 4200 at epoch 0.4154713621525373\n",
      "Logging step 4400 at epoch 0.4352557127312296\n",
      "Logging step 4600 at epoch 0.45504006330992186\n",
      "Logging step 4800 at epoch 0.4748244138886141\n",
      "Logging step 5000 at epoch 0.49460876446730634\n",
      "Logging step 5200 at epoch 0.5143931150459986\n",
      "Logging step 5400 at epoch 0.5341774656246908\n",
      "Logging step 5600 at epoch 0.5539618162033831\n",
      "Logging step 5800 at epoch 0.5737461667820754\n",
      "Logging step 6000 at epoch 0.5935305173607677\n",
      "Logging step 6200 at epoch 0.6133148679394599\n",
      "Logging step 6400 at epoch 0.6330992185181521\n",
      "Logging step 6600 at epoch 0.6528835690968444\n",
      "Logging step 6800 at epoch 0.6726679196755366\n",
      "Logging step 7000 at epoch 0.692452270254229\n",
      "Logging step 7200 at epoch 0.7122366208329212\n",
      "Logging step 7400 at epoch 0.7320209714116134\n",
      "Logging step 7600 at epoch 0.7518053219903057\n",
      "Logging step 7800 at epoch 0.7715896725689979\n",
      "Logging step 8000 at epoch 0.7913740231476902\n",
      "Logging step 8200 at epoch 0.8111583737263824\n",
      "Logging step 8400 at epoch 0.8309427243050747\n",
      "Logging step 8600 at epoch 0.850727074883767\n",
      "Logging step 8800 at epoch 0.8705114254624592\n",
      "Logging step 9000 at epoch 0.8902957760411514\n",
      "Logging step 9200 at epoch 0.9100801266198437\n",
      "Logging step 9400 at epoch 0.9298644771985359\n",
      "Logging step 9600 at epoch 0.9496488277772283\n",
      "Logging step 9800 at epoch 0.9694331783559205\n",
      "Logging step 10000 at epoch 0.9892175289346127\n",
      "Logging step 10200 at epoch 1.009001879513305\n",
      "Logging step 10400 at epoch 1.0287862300919972\n",
      "Logging step 10600 at epoch 1.0485705806706895\n",
      "Logging step 10800 at epoch 1.0683549312493819\n",
      "Logging step 11000 at epoch 1.088139281828074\n",
      "Logging step 11200 at epoch 1.1079236324067663\n",
      "Logging step 11400 at epoch 1.1277079829854584\n",
      "Logging step 11600 at epoch 1.1474923335641507\n",
      "Logging step 11800 at epoch 1.167276684142843\n",
      "Logging step 20600 at epoch 2.037788109605302\n",
      "Logging step 20800 at epoch 2.0575724601839944\n",
      "Logging step 21000 at epoch 2.0773568107626867\n",
      "Logging step 21200 at epoch 2.097141161341379\n",
      "Logging step 21400 at epoch 2.1169255119200714\n",
      "Logging step 21600 at epoch 2.1367098624987637\n",
      "Logging step 21800 at epoch 2.1564942130774556\n",
      "Logging step 22000 at epoch 2.176278563656148\n",
      "Logging step 22200 at epoch 2.1960629142348402\n",
      "Logging step 22400 at epoch 2.2158472648135326\n",
      "Logging step 22600 at epoch 2.235631615392225\n",
      "Logging step 22800 at epoch 2.2554159659709168\n",
      "Logging step 23000 at epoch 2.275200316549609\n",
      "Logging step 23200 at epoch 2.2949846671283014\n",
      "Logging step 23400 at epoch 2.3147690177069937\n",
      "Logging step 23600 at epoch 2.334553368285686\n",
      "Logging step 23800 at epoch 2.3543377188643784\n",
      "Logging step 24000 at epoch 2.3741220694430707\n",
      "Logging step 24200 at epoch 2.3939064200217626\n",
      "Logging step 24400 at epoch 2.413690770600455\n",
      "Logging step 24600 at epoch 2.433475121179147\n",
      "Logging step 24800 at epoch 2.4532594717578395\n",
      "Logging step 25000 at epoch 2.473043822336532\n",
      "Logging step 25200 at epoch 2.492828172915224\n",
      "Logging step 25400 at epoch 2.5126125234939165\n",
      "Logging step 25600 at epoch 2.5323968740726084\n",
      "Logging step 25800 at epoch 2.5521812246513007\n",
      "Logging step 26000 at epoch 2.571965575229993\n",
      "Logging step 26200 at epoch 2.5917499258086854\n",
      "Logging step 26400 at epoch 2.6115342763873777\n",
      "Logging step 26600 at epoch 2.63131862696607\n",
      "Logging step 26800 at epoch 2.6511029775447623\n",
      "Logging step 27000 at epoch 2.670887328123454\n",
      "Logging step 27200 at epoch 2.6906716787021465\n",
      "Logging step 27400 at epoch 2.710456029280839\n",
      "Logging step 27600 at epoch 2.730240379859531\n",
      "Logging step 27800 at epoch 2.7500247304382235\n",
      "Logging step 28000 at epoch 2.7698090810169154\n",
      "Logging step 28200 at epoch 2.789593431595608\n",
      "Logging step 28400 at epoch 2.8093777821743\n",
      "Logging step 28600 at epoch 2.8291621327529923\n",
      "Logging step 28800 at epoch 2.8489464833316847\n",
      "Logging step 29000 at epoch 2.868730833910377\n",
      "Logging step 29200 at epoch 2.8885151844890693\n",
      "Logging step 29400 at epoch 2.908299535067761\n",
      "Logging step 29600 at epoch 2.9280838856464535\n",
      "Logging step 29800 at epoch 2.947868236225146\n",
      "Logging step 30000 at epoch 2.967652586803838\n",
      "Logging step 30200 at epoch 2.9874369373825305\n",
      "Logging step 30400 at epoch 3.007221287961223\n",
      "Logging step 30600 at epoch 3.027005638539915\n",
      "Logging step 30800 at epoch 3.046789989118607\n",
      "Logging step 31000 at epoch 3.0665743396972993\n",
      "Logging step 31200 at epoch 3.0863586902759916\n",
      "Logging step 31400 at epoch 3.106143040854684\n",
      "Logging step 31600 at epoch 3.1259273914333763\n",
      "Logging step 31800 at epoch 3.1457117420120686\n",
      "Logging step 32000 at epoch 3.165496092590761\n",
      "Logging step 32200 at epoch 3.185280443169453\n",
      "Logging step 32400 at epoch 3.205064793748145\n",
      "Logging step 32600 at epoch 3.2248491443268374\n",
      "Logging step 32800 at epoch 3.2446334949055298\n",
      "Logging step 33000 at epoch 3.264417845484222\n",
      "Logging step 33200 at epoch 3.2842021960629144\n",
      "Logging step 33400 at epoch 3.3039865466416067\n",
      "Logging step 33600 at epoch 3.3237708972202986\n",
      "Logging step 33800 at epoch 3.343555247798991\n",
      "Logging step 34000 at epoch 3.3633395983776833\n",
      "Logging step 34200 at epoch 3.3831239489563756\n",
      "Logging step 34400 at epoch 3.402908299535068\n",
      "Logging step 34600 at epoch 3.42269265011376\n",
      "Logging step 34800 at epoch 3.442477000692452\n",
      "Logging step 35000 at epoch 3.4622613512711444\n",
      "Logging step 35200 at epoch 3.4820457018498367\n",
      "Logging step 35400 at epoch 3.501830052428529\n",
      "Logging step 35600 at epoch 3.5216144030072214\n",
      "Logging step 35800 at epoch 3.5413987535859137\n",
      "Logging step 36000 at epoch 3.5611831041646056\n",
      "Logging step 36200 at epoch 3.580967454743298\n",
      "Logging step 36400 at epoch 3.6007518053219902\n",
      "Logging step 36600 at epoch 3.6205361559006826\n",
      "Logging step 36800 at epoch 3.640320506479375\n",
      "Logging step 37000 at epoch 3.660104857058067\n",
      "Logging step 37200 at epoch 3.6798892076367595\n",
      "Logging step 37400 at epoch 3.6996735582154514\n",
      "Logging step 37600 at epoch 3.7194579087941437\n",
      "Logging step 37800 at epoch 3.739242259372836\n",
      "Logging step 38000 at epoch 3.7590266099515284\n",
      "Logging step 38200 at epoch 3.7788109605302207\n",
      "Logging step 38400 at epoch 3.7985953111089126\n",
      "Logging step 38600 at epoch 3.8183796616876053\n",
      "Logging step 38800 at epoch 3.838164012266297\n",
      "Logging step 39000 at epoch 3.8579483628449895\n",
      "Logging step 39200 at epoch 3.877732713423682\n",
      "Logging step 39400 at epoch 3.897517064002374\n",
      "Logging step 39600 at epoch 3.9173014145810665\n",
      "Logging step 39800 at epoch 3.9370857651597584\n",
      "Logging step 40000 at epoch 3.956870115738451\n",
      "Logging step 40200 at epoch 3.976654466317143\n",
      "Logging step 40400 at epoch 3.9964388168958354\n",
      "Logging step 40600 at epoch 4.016223167474528\n",
      "Logging step 40800 at epoch 4.03600751805322\n",
      "Logging step 41000 at epoch 4.055791868631912\n",
      "Logging step 41200 at epoch 4.075576219210604\n",
      "Logging step 41400 at epoch 4.095360569789297\n",
      "Logging step 41600 at epoch 4.115144920367989\n",
      "Logging step 41800 at epoch 4.134929270946682\n",
      "Logging step 42000 at epoch 4.1547136215253735\n",
      "Logging step 42200 at epoch 4.174497972104065\n",
      "Logging step 42400 at epoch 4.194282322682758\n",
      "Logging step 42600 at epoch 4.21406667326145\n",
      "Logging step 42800 at epoch 4.233851023840143\n",
      "Logging step 43000 at epoch 4.253635374418835\n",
      "Logging step 43200 at epoch 4.273419724997527\n",
      "Logging step 43400 at epoch 4.293204075576219\n",
      "Logging step 43600 at epoch 4.312988426154911\n",
      "Logging step 43800 at epoch 4.332772776733604\n",
      "Logging step 44000 at epoch 4.352557127312296\n",
      "Logging step 44200 at epoch 4.372341477890989\n",
      "Logging step 44400 at epoch 4.3921258284696805\n",
      "Logging step 44600 at epoch 4.411910179048372\n",
      "Logging step 44800 at epoch 4.431694529627065\n",
      "Logging step 45000 at epoch 4.451478880205757\n",
      "Logging step 45200 at epoch 4.47126323078445\n",
      "Logging step 45400 at epoch 4.491047581363142\n",
      "Logging step 45600 at epoch 4.5108319319418335\n",
      "Logging step 45800 at epoch 4.530616282520526\n",
      "Logging step 46000 at epoch 4.550400633099218\n",
      "Logging step 46200 at epoch 4.570184983677911\n",
      "Logging step 46400 at epoch 4.589969334256603\n",
      "Logging step 46600 at epoch 4.609753684835296\n",
      "Logging step 46800 at epoch 4.6295380354139875\n",
      "Logging step 47000 at epoch 4.649322385992679\n",
      "Logging step 47200 at epoch 4.669106736571372\n",
      "Logging step 47400 at epoch 4.688891087150064\n",
      "Logging step 47600 at epoch 4.708675437728757\n",
      "Logging step 47800 at epoch 4.728459788307449\n",
      "Logging step 48000 at epoch 4.748244138886141\n",
      "Logging step 48200 at epoch 4.768028489464833\n",
      "Logging step 48400 at epoch 4.787812840043525\n",
      "Logging step 48600 at epoch 4.807597190622218\n",
      "Logging step 48800 at epoch 4.82738154120091\n",
      "Logging step 49000 at epoch 4.8471658917796026\n",
      "Logging step 49200 at epoch 4.866950242358294\n",
      "Logging step 49400 at epoch 4.886734592936987\n",
      "Logging step 49600 at epoch 4.906518943515679\n",
      "Logging step 49800 at epoch 4.926303294094371\n",
      "Logging step 50000 at epoch 4.946087644673064\n",
      "Logging step 50200 at epoch 4.965871995251756\n",
      "Logging step 50400 at epoch 4.985656345830448\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f23cf2-19b4-4832-aa51-e62040749cc0",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00e32844-44df-430c-a298-0a00e58aafef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-fs/t5-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('LoRA_results_t5_large_4/tokenizer_config.json',\n",
       " 'LoRA_results_t5_large_4/special_tokens_map.json',\n",
       " 'LoRA_results_t5_large_4/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "'''========================================'''\n",
    "peft_model_id=\"LoRA_results_t5_large_4\"\n",
    "'''========================================'''\n",
    "\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "# if you want to save the base model to call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d108b-711f-498b-b6d7-907b7110de63",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fb819-081c-44fa-9fb1-dd6c22175fea",
   "metadata": {},
   "source": [
    "# <center><font color=Green size=20>Evaluate (Example Answers)</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7b6566f-b9e7-4d5f-a3c1-79784a41f21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "'''================================================'''\n",
    "peft_model_id = \"LoRA_results_t5_large_4\"\n",
    "'''================================================'''\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "if config.base_model_name_or_path:\n",
    "    ;\n",
    "else:\n",
    "    '''========================================================'''\n",
    "    config.base_model_name_or_path = \"/root/autodl-fs/t5-large\"\n",
    "    '''========================================================'''\n",
    "    \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "    \n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "957733f3-638f-456d-bd4f-057276d9fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input question: does this lamp need wiring from a boz or comes with a plug\n",
      "------------------------------------------------------------\n",
      "['<pad> No it comes with a plug. The switch is the same size and shape as the Bizo that is shown in the pictures.</s>']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# # Load dataset from the hub and get a sample\n",
    "data_files = {\"train\": \"train.json\", \"test\":\"test.json\", \"val\":\"valid.json\"}\n",
    "dataset = load_dataset('json', data_dir=\"./Data/\", data_files=data_files)\n",
    "\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "\n",
    "input_ids = tokenizer(sample[\"question\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# # with torch.inference_mode():\n",
    "# outputs = model.generate(input_ids=input_ids, max_new_tokens=512, do_sample=True, top_p=0.9)\n",
    "print(f\"input question: {sample['question']}\\n{'---'* 20}\")\n",
    "\n",
    "output = model.generate(input_ids=input_ids, max_new_tokens=128, output_hidden_states=True, output_scores=True,return_dict_in_generate=True, do_sample=True, top_p=0.9)\n",
    "#print(f\"----------{output}\")\n",
    "#全部的输出\n",
    "decoded_output = [tokenizer.decode(ids) for ids in output.sequences]\n",
    "print(decoded_output)\n",
    "#####################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c88c01e-1691-43a0-979d-73a6ccede898",
   "metadata": {},
   "source": [
    "# <center><font color=Green size=20>Evaluate (Metrics)</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b021f-66f3-4fbb-834e-e8ff278d0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"./evaluate-main/metrics/rouge\")\n",
    "\n",
    "'''========================================================Evaluate PEFT model (function)==============================================================='''\n",
    "def evaluate_peft_model(sample,max_target_length=512):\n",
    "    # generate summary\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    # decode eval sample\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels\n",
    "''' ========================================================================================================================================='''\n",
    "# load test dataset from disk\n",
    "test_dataset = load_from_disk(\"./Data/test/\").with_format(\"torch\")\n",
    "examples = test_dataset.select(range(5))\n",
    "\n",
    "# run predictions\n",
    "# this can take ~45 minutes\n",
    "predictions, references = [] , []\n",
    "\n",
    "# for sample in tqdm(examples):\n",
    "for sample in tqdm(test_dataset):\n",
    "    #print(sample.keys())\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "    predictions.append(p)\n",
    "    references.append(l)\n",
    "\n",
    "# compute metric \n",
    "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "# print results \n",
    "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
    "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
    "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
    "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n",
    "\n",
    "# write to wandb log\n",
    "import wandb\n",
    "\n",
    "'''========================================================='''\n",
    "project_name = \"LLM_LoRA_FineTuning\"\n",
    "\n",
    "'''========================================================='''\n",
    "run_id = \"tnnnrg0n\"                                         # 可以是一个唯一标识符，如果已经有运行 ID\n",
    "'''========================================================='''\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "run = api.run(f\"llm_learner/{project_name}/{run_id}\")\n",
    "run.config[\"test_Rogue1\"] = rogue['rouge1']\n",
    "run.config[\"test_Rogue2\"] = rogue['rouge2']\n",
    "run.config[\"test_RogueL\"] = rogue['rougeL']\n",
    "run.config[\"test_RogueLsum\"] = rogue['rougeLsum']\n",
    "run.update()\n",
    "\n",
    "wandb.init(project=project_name, id=run_id, resume=\"allow\")\n",
    "wandb.log({\"test_Rogue1\": rogue['rouge1'], \"test_Rogue2\": rogue['rouge2'], \"test_RogueL\": rogue['rougeL'], \"test_RogueLsum\": rogue['rougeLsum']})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
