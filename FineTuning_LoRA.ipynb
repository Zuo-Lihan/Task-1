{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0da33-493a-4314-93a4-cdf1355f1adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cce9f6b6-f2b2-407e-85fa-ced2d1df64b6",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9510b818-0e27-412e-b11a-820c6d38227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 80870\n",
      "Test dataset size: 10110\n",
      "Valid dataset size: 10108\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_name = \"train.json\"\n",
    "valid_name = \"valid.json\"\n",
    "test_name = \"test.json\"\n",
    "data_dir = \"./Data\"\n",
    "\n",
    "data_files = {\"train\": train_name, \"test\": test_name, \"valid\": valid_name}\n",
    "dataset = load_dataset('json', data_dir = data_dir, data_files = data_files)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "print(f\"Valid dataset size: {len(dataset['valid'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62182466-67c8-4837-b481-e7a7376dc08b",
   "metadata": {},
   "source": [
    "# Preparation before training\n",
    "\n",
    "Before the training of LLM, we need to do preliminary-disposition of dataset.\n",
    "GQA belongs to the Text-Generation task.\n",
    "We need to know the length information of the input & output text, which will benefit for the high-efficient batch-processing for these dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef009b98-8c26-4055-ab48-9495f260a813",
   "metadata": {},
   "source": [
    "* We utilize the t5-large model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b925582d-8741-44c1-98a5-3ca437bba26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_path = \"/root/autodl-fs/flan-t5-xxl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adad5c1-4d19-4244-b3cf-cafb4d354be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX source length: 34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80231cc68a55411ab69614091a28d814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX target length: 90\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than the max will be truncated, and sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"question\"], truncation=True), batched = True, remove_columns=[\"question\", \"answer\"])\n",
    "input_lengths = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lengths, 85))\n",
    "print(f\"MAX source length: {max_source_length}\")\n",
    "\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"answer\"], truncation=True), batched=True, remove_columns=[\"question\", \"answer\"])\n",
    "target_lengths = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lengths, 90))\n",
    "print(f\"MAX target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f267c-2243-4cfe-8136-4c3d18ce8752",
   "metadata": {},
   "source": [
    "## We do pre-processing for all dataset and save the processed dataset to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8a38d4-b786-4b99-9ada-fc62bd3a5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(samples, padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [item for item in samples[\"question\"]]\n",
    "    \n",
    "    # tokenize the inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "    \n",
    "    # tokenize targets with the 'text_target' keyword argument\n",
    "    labels = tokenizer(text_target=samples[\"answer\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "    \n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "        \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d1bd88-9027-4287-b622-886ba9d43fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d1b44137884aab83b1c1256f7503c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9207aacaa1a41aab014a7bb4836bd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/80870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f28d97f5efb40ecbbf1d5030aa6eddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns = [\"question\", \"answer\", \"question_type\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"Data/Train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"Data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f09405-6538-41ed-8129-9ce00e6fe1e9",
   "metadata": {},
   "source": [
    "# LoRA & bnb-int8 to Fine-tuning the T5-large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da0c664-1a0f-4f00-8911-9b8ae552610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"/root/autodl-fs/t5-large\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72f23c7-13b7-4ef4-9c0f-0ba26103e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1: trainable params: 3,538,944 || all params: 741,207,040 || trainable%: 0.4774568789848515\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/other.py:147: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training,TaskType\n",
    "import wandb\n",
    "config1 = {\"r\": 12,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"target_modules\": [\"q\", \"v\"],\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": TaskType.SEQ_2_SEQ_LM\n",
    "         }\n",
    "\n",
    "\n",
    "# define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r = config1[\"r\"],\n",
    "    lora_alpha = config1[\"lora_alpha\"],\n",
    "    target_modules = config1[\"target_modules\"],\n",
    "    lora_dropout = config1[\"lora_dropout\"],\n",
    "    bias = config1[\"bias\"],\n",
    "    task_type = config1[\"task_type\"]\n",
    ")\n",
    "\n",
    "\n",
    "# prepare int8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adapter\n",
    "model = get_peft_model(model, lora_config)\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# ÂÅáËÆæ model ÊòØÊÇ®Â∑≤ÁªèÂÆö‰πâÂ•ΩÁöÑÊ®°ÂûãÂØπË±°\n",
    "\n",
    "# ÂàõÂª∫‰∏Ä‰∏™ StringIO ÂØπË±°\n",
    "output = io.StringIO()\n",
    "\n",
    "# ‰øùÂ≠òÂΩìÂâçÁöÑ stdout\n",
    "original_stdout = sys.stdout\n",
    "\n",
    "try:\n",
    "    # Â∞Ü stdout ÈáçÂÆöÂêëÂà∞ StringIO ÂØπË±°\n",
    "    sys.stdout = output\n",
    "    # Ë∞ÉÁî®ÊñπÊ≥ïÔºåÊâìÂç∞ËæìÂá∫Âà∞ StringIO ÂØπË±°\n",
    "    model.print_trainable_parameters()\n",
    "finally:\n",
    "    # ÊÅ¢Â§çÂéüÂßãÁöÑ stdout\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "# Ëé∑Âèñ StringIO ÂØπË±°‰∏≠ÁöÑÂÜÖÂÆπ\n",
    "output_str = output.getvalue()\n",
    "\n",
    "# ÂÖ≥Èó≠ StringIO ÂØπË±°\n",
    "output.close()\n",
    "\n",
    "# ÊâìÂç∞ÊçïËé∑Âà∞ÁöÑÂ≠óÁ¨¶‰∏≤ÂÜÖÂÆπ\n",
    "print(\"1:\",output_str)\n",
    "\n",
    "# wandb.init(\n",
    "#     project = \"LLM_LoRA_FineTuning\",\n",
    "    \n",
    "#     config = {\n",
    "#         \"config\": config1,\n",
    "#         \"Dataset\": \"qa_Tools_and_Home_Improvement\",\n",
    "#         \"Tuning-method\": \"LoRA\",\n",
    "#         \"Trainable params\": str(output_str)\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61f560-63af-4899-9ceb-fefa7aca7c79",
   "metadata": {},
   "source": [
    "Êé•‰∏ãÊù•ÈúÄË¶ÅÂàõÂª∫‰∏Ä‰∏™ $DataCollator$ÔºåË¥üË¥£ÂØπËæìÂÖ•ÂíåÊ†áÁ≠æËøõË°åÂ°´ÂÖÖÔºåÊàë‰ª¨‰ΩøÁî® ü§ó $Transformers$ Â∫ì‰∏≠ÁöÑ$DataCollatorForSeq2Seq$ Êù•ÂÆåÊàêËøô‰∏ÄÁéØËäÇ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e27782-5555-499c-9737-717bd91c7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100 \n",
    "# Âú®ËÆ≠ÁªÉÂ∫èÂàóÂà∞Â∫èÂàóÔºàSeq2SeqÔºâÊ®°ÂûãÊó∂ÔºåÈÄöÂ∏∏‰ºö‰ΩøÁî®ÁâπÊÆäÁöÑÂ°´ÂÖÖÊ†áËÆ∞Êù•ÂØπËæìÂÖ•ËøõË°åÂ§ÑÁêÜ„ÄÇÂØπ‰∫éÊ†áÁ≠æÊï∞ÊçÆÔºåÂú®ËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞Êó∂ÔºåÊàë‰ª¨ÈúÄË¶ÅÂøΩÁï•Â°´ÂÖÖÊ†áËÆ∞ÊâÄÂ∏¶Êù•ÁöÑÂΩ±ÂìçÔºåÂõ†‰∏∫Ëøô‰∫õÂ°´ÂÖÖÈÉ®ÂàÜ‰∏çÂ∫îËØ•ÂèÇ‰∏éÂà∞ÊçüÂ§±ÁöÑËÆ°ÁÆó‰∏≠„ÄÇ\n",
    "# Âú® Transformers Â∫ì‰∏≠ÔºåÈÄöÂ∏∏Â∞Ü‰∏çÂ∫îËØ•Ë¢´ËÄÉËôëÁöÑÊ†áÁ≠æËÆæÁΩÆ‰∏∫‰∏Ä‰∏™ÁâπÂÆöÁöÑÂÄºÔºåÈÄöÂ∏∏ÊòØ -100„ÄÇÂΩìËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞Êó∂ÔºåÊ®°Âûã‰ºöÂøΩÁï•Ëøô‰∫õ -100 ÂÄºÊâÄÂØπÂ∫îÁöÑÈ¢ÑÊµãÁªìÊûúÔºåÂè™ËÆ°ÁÆóÁúüÂÆûÊ†áÁ≠æÈÉ®ÂàÜÁöÑÊçüÂ§±ÂÄºÔºå‰ªéËÄåÂÆûÁé∞Âú®ËÆ°ÁÆóÊçüÂ§±ÂáΩÊï∞Êó∂ÂøΩÁï•Â°´ÂÖÖÊ†áËÆ∞ÁöÑÊïàÊûú„ÄÇ \n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b3801-7432-4390-9a21-ffea1df2d857",
   "metadata": {},
   "source": [
    "Ëá™ÂÆö‰πâÂõûË∞ÉÂáΩÊï∞Êù•ËÆ∞ÂΩïËÆ≠ÁªÉÊçüÂ§±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f5e639-628c-401c-9555-4503714befdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "class TrainLogCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if 'loss' in state.log_history[-1]:\n",
    "            self.train_losses.append(state.log_history[-1]['loss'])\n",
    "            if state.global_step % args.logging_steps == 0:\n",
    "                print(f\"Logging step {state.global_step} at epoch {state.epoch}\")\n",
    "                wandb.log({\"logging_step\": state.global_step, \"train_loss\": state.log_history[-1]['loss']})\n",
    "            # print(f\"Step: {state.global_step}, Train Loss: {state.log_history[-1]['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d53dd30f-8091-439a-8ec1-2e7eb65b0f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzuolihanstudy\u001b[0m (\u001b[33mllm_learner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/Task-1/wandb/run-20240612_143906-ckpsqjjb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb' target=\"_blank\">lucky-bird-12</a></strong> to <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"t5-large\"\n",
    "\n",
    "config2 = {\n",
    "    \"output_dir\": output_dir,\n",
    "    \"auto_find_batch_size\": True,\n",
    "    \"learning_rate\": 1.5e-3,\n",
    "    \"num_train_epochs\":5,\n",
    "    \"logging_dir\":f\"{output_dir}/logs\",\n",
    "    \"logging_strategy\":\"steps\",\n",
    "    \"logging_steps\":200,\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"report_to\": \"tensorboard\"\n",
    "}\n",
    "\n",
    "# ÂàùÂßãÂåñËá™ÂÆö‰πâÂõûË∞É\n",
    "log_step_callback = TrainLogCallback()\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = config2[\"output_dir\"],\n",
    "    auto_find_batch_size = config2[\"auto_find_batch_size\"],\n",
    "    learning_rate = config2[\"learning_rate\"], # higher learning rate\n",
    "    num_train_epochs = config2[\"num_train_epochs\"],\n",
    "    logging_dir = config2[\"logging_dir\"],\n",
    "    logging_strategy = config2[\"logging_strategy\"],\n",
    "    logging_steps = config2[\"logging_steps\"],\n",
    "    save_strategy = config2[\"save_strategy\"],\n",
    "    report_to = config2[\"report_to\"],\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project = \"LLM_LoRA_FineTuning\",\n",
    "    \n",
    "    config = {\n",
    "        \"config1\": config1,\n",
    "        \"config2\": config2,\n",
    "        \"Dataset\": \"qa_Tools_and_Home_Improvement\",\n",
    "        \"Tuning-method\": \"LoRA\",\n",
    "        \"Trainable params\": str(output_str)\n",
    "    }\n",
    ")\n",
    "\n",
    "# api = wandb.Api()\n",
    "\n",
    "# run = api.run(\"llm_learner/LLM_LoRA_FineTuning/623waqcp\")\n",
    "# run.config[\"config2\"] = config2\n",
    "# run.update()\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    callbacks=[log_step_callback]\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b2a41-28e9-440d-9fac-a54d9d143492",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36054041-e460-48b7-92c8-8ea17bdea7dc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50545' max='50545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50545/50545 11:30:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.380200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.330300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.319900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.333300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.297400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.354100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.305600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.350600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.313800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.334600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.324900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.361800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>3.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>3.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>3.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.356900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.331700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>3.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>3.376200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>3.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>3.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>3.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>3.286100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>3.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>3.328500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>3.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>3.303400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>3.283700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>3.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>3.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>3.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>3.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>3.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>3.277400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>3.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>3.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>3.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.308000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>3.264600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>3.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>3.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>3.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>3.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>3.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>3.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>3.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>3.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>3.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>3.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>3.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.323500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>3.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>3.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>3.303300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>3.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>3.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>3.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>3.339500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>3.286800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.295400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>3.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>3.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.284600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>3.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>3.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>3.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>3.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>3.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>3.257800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>3.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>3.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>3.273100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>3.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>3.263900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>3.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>3.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>3.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>3.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>3.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>3.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>3.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>3.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>3.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>3.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>3.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>3.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>3.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>3.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>3.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>3.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>3.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>3.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>3.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>3.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>3.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>3.242700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>3.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>3.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>3.220300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>3.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>3.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>3.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>3.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>3.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>3.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>3.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>3.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>3.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>3.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>3.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>3.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>3.225800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>3.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>3.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>3.204400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>3.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.172100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>3.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>3.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>3.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>3.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>3.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>3.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>3.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>3.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>3.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>3.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>3.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>3.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>3.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>3.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>3.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>3.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>3.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>3.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>3.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>3.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>3.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>3.159600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>3.151100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>3.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>3.179300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39400</td>\n",
       "      <td>3.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>3.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39800</td>\n",
       "      <td>3.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>3.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40400</td>\n",
       "      <td>3.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40600</td>\n",
       "      <td>3.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>3.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41200</td>\n",
       "      <td>3.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>3.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41600</td>\n",
       "      <td>3.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41800</td>\n",
       "      <td>3.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42200</td>\n",
       "      <td>3.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42400</td>\n",
       "      <td>3.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42600</td>\n",
       "      <td>3.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42800</td>\n",
       "      <td>3.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.080700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43200</td>\n",
       "      <td>3.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43400</td>\n",
       "      <td>3.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43600</td>\n",
       "      <td>3.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43800</td>\n",
       "      <td>3.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44200</td>\n",
       "      <td>3.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44400</td>\n",
       "      <td>3.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44600</td>\n",
       "      <td>3.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44800</td>\n",
       "      <td>3.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45200</td>\n",
       "      <td>3.116700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45400</td>\n",
       "      <td>3.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45600</td>\n",
       "      <td>3.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45800</td>\n",
       "      <td>3.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>3.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46200</td>\n",
       "      <td>3.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46400</td>\n",
       "      <td>3.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46600</td>\n",
       "      <td>3.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46800</td>\n",
       "      <td>3.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>3.109700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47200</td>\n",
       "      <td>3.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47400</td>\n",
       "      <td>3.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47600</td>\n",
       "      <td>3.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47800</td>\n",
       "      <td>3.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>3.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48200</td>\n",
       "      <td>3.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48400</td>\n",
       "      <td>3.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48600</td>\n",
       "      <td>3.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48800</td>\n",
       "      <td>3.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>3.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49200</td>\n",
       "      <td>3.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49400</td>\n",
       "      <td>3.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49600</td>\n",
       "      <td>3.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49800</td>\n",
       "      <td>3.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50200</td>\n",
       "      <td>3.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50400</td>\n",
       "      <td>3.068800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging step 200 at epoch 0.019784350578692253\n",
      "Logging step 400 at epoch 0.039568701157384506\n",
      "Logging step 600 at epoch 0.059353051736076766\n",
      "Logging step 800 at epoch 0.07913740231476901\n",
      "Logging step 1000 at epoch 0.09892175289346128\n",
      "Logging step 1200 at epoch 0.11870610347215353\n",
      "Logging step 1400 at epoch 0.13849045405084578\n",
      "Logging step 1600 at epoch 0.15827480462953802\n",
      "Logging step 1800 at epoch 0.1780591552082303\n",
      "Logging step 2000 at epoch 0.19784350578692256\n",
      "Logging step 2200 at epoch 0.2176278563656148\n",
      "Logging step 2400 at epoch 0.23741220694430706\n",
      "Logging step 2600 at epoch 0.2571965575229993\n",
      "Logging step 2800 at epoch 0.27698090810169157\n",
      "Logging step 3000 at epoch 0.29676525868038384\n",
      "Logging step 3200 at epoch 0.31654960925907605\n",
      "Logging step 3400 at epoch 0.3363339598377683\n",
      "Logging step 3600 at epoch 0.3561183104164606\n",
      "Logging step 3800 at epoch 0.37590266099515285\n",
      "Logging step 4000 at epoch 0.3956870115738451\n",
      "Logging step 4200 at epoch 0.4154713621525373\n",
      "Logging step 4400 at epoch 0.4352557127312296\n",
      "Logging step 4600 at epoch 0.45504006330992186\n",
      "Logging step 4800 at epoch 0.4748244138886141\n",
      "Logging step 5000 at epoch 0.49460876446730634\n",
      "Logging step 5200 at epoch 0.5143931150459986\n",
      "Logging step 5400 at epoch 0.5341774656246908\n",
      "Logging step 5600 at epoch 0.5539618162033831\n",
      "Logging step 5800 at epoch 0.5737461667820754\n",
      "Logging step 6000 at epoch 0.5935305173607677\n",
      "Logging step 6200 at epoch 0.6133148679394599\n",
      "Logging step 6400 at epoch 0.6330992185181521\n",
      "Logging step 6600 at epoch 0.6528835690968444\n",
      "Logging step 6800 at epoch 0.6726679196755366\n",
      "Logging step 7000 at epoch 0.692452270254229\n",
      "Logging step 7200 at epoch 0.7122366208329212\n",
      "Logging step 7400 at epoch 0.7320209714116134\n",
      "Logging step 7600 at epoch 0.7518053219903057\n",
      "Logging step 7800 at epoch 0.7715896725689979\n",
      "Logging step 8000 at epoch 0.7913740231476902\n",
      "Logging step 8200 at epoch 0.8111583737263824\n",
      "Logging step 8400 at epoch 0.8309427243050747\n",
      "Logging step 8600 at epoch 0.850727074883767\n",
      "Logging step 8800 at epoch 0.8705114254624592\n",
      "Logging step 9000 at epoch 0.8902957760411514\n",
      "Logging step 9200 at epoch 0.9100801266198437\n",
      "Logging step 9400 at epoch 0.9298644771985359\n",
      "Logging step 9600 at epoch 0.9496488277772283\n",
      "Logging step 9800 at epoch 0.9694331783559205\n",
      "Logging step 10000 at epoch 0.9892175289346127\n",
      "Logging step 10200 at epoch 1.009001879513305\n",
      "Logging step 10400 at epoch 1.0287862300919972\n",
      "Logging step 10600 at epoch 1.0485705806706895\n",
      "Logging step 10800 at epoch 1.0683549312493819\n",
      "Logging step 11000 at epoch 1.088139281828074\n",
      "Logging step 11200 at epoch 1.1079236324067663\n",
      "Logging step 11400 at epoch 1.1277079829854584\n",
      "Logging step 11600 at epoch 1.1474923335641507\n",
      "Logging step 11800 at epoch 1.167276684142843\n",
      "Logging step 12000 at epoch 1.1870610347215353\n",
      "Logging step 12200 at epoch 1.2068453853002274\n",
      "Logging step 12400 at epoch 1.2266297358789198\n",
      "Logging step 12600 at epoch 1.246414086457612\n",
      "Logging step 12800 at epoch 1.2661984370363042\n",
      "Logging step 13000 at epoch 1.2859827876149965\n",
      "Logging step 13200 at epoch 1.3057671381936888\n",
      "Logging step 13400 at epoch 1.3255514887723812\n",
      "Logging step 13600 at epoch 1.3453358393510733\n",
      "Logging step 13800 at epoch 1.3651201899297656\n",
      "Logging step 14000 at epoch 1.3849045405084577\n",
      "Logging step 14200 at epoch 1.40468889108715\n",
      "Logging step 14400 at epoch 1.4244732416658423\n",
      "Logging step 14600 at epoch 1.4442575922445346\n",
      "Logging step 14800 at epoch 1.4640419428232267\n",
      "Logging step 15000 at epoch 1.483826293401919\n",
      "Logging step 15200 at epoch 1.5036106439806112\n",
      "Logging step 15400 at epoch 1.5233949945593035\n",
      "Logging step 15600 at epoch 1.5431793451379958\n",
      "Logging step 15800 at epoch 1.5629636957166881\n",
      "Logging step 16000 at epoch 1.5827480462953805\n",
      "Logging step 16200 at epoch 1.6025323968740726\n",
      "Logging step 16400 at epoch 1.6223167474527649\n",
      "Logging step 16600 at epoch 1.642101098031457\n",
      "Logging step 16800 at epoch 1.6618854486101493\n",
      "Logging step 17000 at epoch 1.6816697991888416\n",
      "Logging step 17200 at epoch 1.701454149767534\n",
      "Logging step 17400 at epoch 1.7212385003462263\n",
      "Logging step 17600 at epoch 1.7410228509249184\n",
      "Logging step 17800 at epoch 1.7608072015036107\n",
      "Logging step 18000 at epoch 1.7805915520823028\n",
      "Logging step 18200 at epoch 1.8003759026609951\n",
      "Logging step 18400 at epoch 1.8201602532396874\n",
      "Logging step 18600 at epoch 1.8399446038183798\n",
      "Logging step 18800 at epoch 1.8597289543970719\n",
      "Logging step 19000 at epoch 1.8795133049757642\n",
      "Logging step 19200 at epoch 1.8992976555544563\n",
      "Logging step 19400 at epoch 1.9190820061331486\n",
      "Logging step 19600 at epoch 1.938866356711841\n",
      "Logging step 19800 at epoch 1.9586507072905333\n",
      "Logging step 20000 at epoch 1.9784350578692256\n",
      "Logging step 20200 at epoch 1.9982194084479177\n",
      "Logging step 20400 at epoch 2.01800375902661\n",
      "Logging step 20600 at epoch 2.037788109605302\n",
      "Logging step 20800 at epoch 2.0575724601839944\n",
      "Logging step 21000 at epoch 2.0773568107626867\n",
      "Logging step 21200 at epoch 2.097141161341379\n",
      "Logging step 21400 at epoch 2.1169255119200714\n",
      "Logging step 21600 at epoch 2.1367098624987637\n",
      "Logging step 21800 at epoch 2.1564942130774556\n",
      "Logging step 22000 at epoch 2.176278563656148\n",
      "Logging step 22200 at epoch 2.1960629142348402\n",
      "Logging step 22400 at epoch 2.2158472648135326\n",
      "Logging step 22600 at epoch 2.235631615392225\n",
      "Logging step 22800 at epoch 2.2554159659709168\n",
      "Logging step 23000 at epoch 2.275200316549609\n",
      "Logging step 23200 at epoch 2.2949846671283014\n",
      "Logging step 23400 at epoch 2.3147690177069937\n",
      "Logging step 23600 at epoch 2.334553368285686\n",
      "Logging step 23800 at epoch 2.3543377188643784\n",
      "Logging step 24000 at epoch 2.3741220694430707\n",
      "Logging step 24200 at epoch 2.3939064200217626\n",
      "Logging step 24400 at epoch 2.413690770600455\n",
      "Logging step 24600 at epoch 2.433475121179147\n",
      "Logging step 24800 at epoch 2.4532594717578395\n",
      "Logging step 25000 at epoch 2.473043822336532\n",
      "Logging step 25200 at epoch 2.492828172915224\n",
      "Logging step 25400 at epoch 2.5126125234939165\n",
      "Logging step 25600 at epoch 2.5323968740726084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging step 26200 at epoch 2.5917499258086854\n",
      "Logging step 26400 at epoch 2.6115342763873777\n",
      "Logging step 26600 at epoch 2.63131862696607\n",
      "Logging step 26800 at epoch 2.6511029775447623\n",
      "Logging step 27000 at epoch 2.670887328123454\n",
      "Logging step 27200 at epoch 2.6906716787021465\n",
      "Logging step 27400 at epoch 2.710456029280839\n",
      "Logging step 27600 at epoch 2.730240379859531\n",
      "Logging step 27800 at epoch 2.7500247304382235\n",
      "Logging step 28000 at epoch 2.7698090810169154\n",
      "Logging step 28200 at epoch 2.789593431595608\n",
      "Logging step 28400 at epoch 2.8093777821743\n",
      "Logging step 28600 at epoch 2.8291621327529923\n",
      "Logging step 28800 at epoch 2.8489464833316847\n",
      "Logging step 29000 at epoch 2.868730833910377\n",
      "Logging step 29200 at epoch 2.8885151844890693\n",
      "Logging step 29400 at epoch 2.908299535067761\n",
      "Logging step 29600 at epoch 2.9280838856464535\n",
      "Logging step 29800 at epoch 2.947868236225146\n",
      "Logging step 30000 at epoch 2.967652586803838\n",
      "Logging step 30200 at epoch 2.9874369373825305\n",
      "Logging step 30600 at epoch 3.027005638539915\n",
      "Logging step 30800 at epoch 3.046789989118607\n",
      "Logging step 31000 at epoch 3.0665743396972993\n",
      "Logging step 31200 at epoch 3.0863586902759916\n",
      "Logging step 31400 at epoch 3.106143040854684\n",
      "Logging step 31600 at epoch 3.1259273914333763\n",
      "Logging step 31800 at epoch 3.1457117420120686\n",
      "Logging step 32200 at epoch 3.185280443169453\n",
      "Logging step 32400 at epoch 3.205064793748145\n",
      "Logging step 32600 at epoch 3.2248491443268374\n",
      "Logging step 32800 at epoch 3.2446334949055298\n",
      "Logging step 33000 at epoch 3.264417845484222\n",
      "Logging step 33200 at epoch 3.2842021960629144\n",
      "Logging step 33400 at epoch 3.3039865466416067\n",
      "Logging step 33600 at epoch 3.3237708972202986\n",
      "Logging step 33800 at epoch 3.343555247798991\n",
      "Logging step 34000 at epoch 3.3633395983776833\n",
      "Logging step 34200 at epoch 3.3831239489563756\n",
      "Logging step 34400 at epoch 3.402908299535068\n",
      "Logging step 34600 at epoch 3.42269265011376\n",
      "Logging step 34800 at epoch 3.442477000692452\n",
      "Logging step 35000 at epoch 3.4622613512711444\n",
      "Logging step 35200 at epoch 3.4820457018498367\n",
      "Logging step 35400 at epoch 3.501830052428529\n",
      "Logging step 35600 at epoch 3.5216144030072214\n",
      "Logging step 35800 at epoch 3.5413987535859137\n",
      "Logging step 36000 at epoch 3.5611831041646056\n",
      "Logging step 36200 at epoch 3.580967454743298\n",
      "Logging step 36400 at epoch 3.6007518053219902\n",
      "Logging step 36600 at epoch 3.6205361559006826\n",
      "Logging step 36800 at epoch 3.640320506479375\n",
      "Logging step 37000 at epoch 3.660104857058067\n",
      "Logging step 37200 at epoch 3.6798892076367595\n",
      "Logging step 37400 at epoch 3.6996735582154514\n",
      "Logging step 37600 at epoch 3.7194579087941437\n",
      "Logging step 37800 at epoch 3.739242259372836\n",
      "Logging step 38000 at epoch 3.7590266099515284\n",
      "Logging step 38200 at epoch 3.7788109605302207\n",
      "Logging step 38400 at epoch 3.7985953111089126\n",
      "Logging step 38600 at epoch 3.8183796616876053\n",
      "Logging step 38800 at epoch 3.838164012266297\n",
      "Logging step 39000 at epoch 3.8579483628449895\n",
      "Logging step 39200 at epoch 3.877732713423682\n",
      "Logging step 39400 at epoch 3.897517064002374\n",
      "Logging step 39600 at epoch 3.9173014145810665\n",
      "Logging step 39800 at epoch 3.9370857651597584\n",
      "Logging step 40000 at epoch 3.956870115738451\n",
      "Logging step 40200 at epoch 3.976654466317143\n",
      "Logging step 40400 at epoch 3.9964388168958354\n",
      "Logging step 40600 at epoch 4.016223167474528\n",
      "Logging step 40800 at epoch 4.03600751805322\n",
      "Logging step 41000 at epoch 4.055791868631912\n",
      "Logging step 41200 at epoch 4.075576219210604\n",
      "Logging step 41400 at epoch 4.095360569789297\n",
      "Logging step 41600 at epoch 4.115144920367989\n",
      "Logging step 41800 at epoch 4.134929270946682\n",
      "Logging step 42000 at epoch 4.1547136215253735\n",
      "Logging step 42200 at epoch 4.174497972104065\n",
      "Logging step 42400 at epoch 4.194282322682758\n",
      "Logging step 42600 at epoch 4.21406667326145\n",
      "Logging step 42800 at epoch 4.233851023840143\n",
      "Logging step 43000 at epoch 4.253635374418835\n",
      "Logging step 43200 at epoch 4.273419724997527\n",
      "Logging step 43400 at epoch 4.293204075576219\n",
      "Logging step 43600 at epoch 4.312988426154911\n",
      "Logging step 43800 at epoch 4.332772776733604\n",
      "Logging step 44000 at epoch 4.352557127312296\n",
      "Logging step 44200 at epoch 4.372341477890989\n",
      "Logging step 44400 at epoch 4.3921258284696805\n",
      "Logging step 44600 at epoch 4.411910179048372\n",
      "Logging step 44800 at epoch 4.431694529627065\n",
      "Logging step 45000 at epoch 4.451478880205757\n",
      "Logging step 45200 at epoch 4.47126323078445\n",
      "Logging step 45400 at epoch 4.491047581363142\n",
      "Logging step 45600 at epoch 4.5108319319418335\n",
      "Logging step 45800 at epoch 4.530616282520526\n",
      "Logging step 46000 at epoch 4.550400633099218\n",
      "Logging step 46200 at epoch 4.570184983677911\n",
      "Logging step 46400 at epoch 4.589969334256603\n",
      "Logging step 46600 at epoch 4.609753684835296\n",
      "Logging step 46800 at epoch 4.6295380354139875\n",
      "Logging step 47000 at epoch 4.649322385992679\n",
      "Logging step 47200 at epoch 4.669106736571372\n",
      "Logging step 47400 at epoch 4.688891087150064\n",
      "Logging step 47600 at epoch 4.708675437728757\n",
      "Logging step 47800 at epoch 4.728459788307449\n",
      "Logging step 48000 at epoch 4.748244138886141\n",
      "Logging step 48200 at epoch 4.768028489464833\n",
      "Logging step 48400 at epoch 4.787812840043525\n",
      "Logging step 48600 at epoch 4.807597190622218\n",
      "Logging step 48800 at epoch 4.82738154120091\n",
      "Logging step 49000 at epoch 4.8471658917796026\n",
      "Logging step 49200 at epoch 4.866950242358294\n",
      "Logging step 49400 at epoch 4.886734592936987\n",
      "Logging step 49600 at epoch 4.906518943515679\n",
      "Logging step 49800 at epoch 4.926303294094371\n",
      "Logging step 50000 at epoch 4.946087644673064\n",
      "Logging step 50200 at epoch 4.965871995251756\n",
      "Logging step 50400 at epoch 4.985656345830448\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b45c0f6-cb63-448b-be42-fd2040ae33ff",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed36c809-10d4-495f-abb4-8327df80b1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-fs/t5-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('LoRA_results_t5_large_1/tokenizer_config.json',\n",
       " 'LoRA_results_t5_large_1/special_tokens_map.json',\n",
       " 'LoRA_results_t5_large_1/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "peft_model_id=\"LoRA_results_t5_large_1\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "# if you want to save the base model to call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc1218-831c-4309-bcd6-a37e15dd54da",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51fe94-cea3-4e69-9cb5-7ca062d83d0e",
   "metadata": {},
   "source": [
    "# <center><font color=Green size=20>Evaluate (Example Answers)</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9d37d1-d67a-4cef-93ab-a656f3597b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "peft_model_id = \"LoRA_results_t5_large_1\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e093f40-e455-4fd6-a256-028eaff7c508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input question: How does it work on carpet?\n",
      "------------------------------------------------------------\n",
      "['<pad> It works better on carpet than on other surface, although carpet usually looks better to me. My carpet is very smooth on the walls.</s>']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# # Load dataset from the hub and get a sample\n",
    "data_files = {\"train\": \"train.json\", \"test\":\"test.json\", \"val\":\"valid.json\"}\n",
    "dataset = load_dataset('json', data_dir=\"./Data/\", data_files=data_files)\n",
    "\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "\n",
    "input_ids = tokenizer(sample[\"question\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# # with torch.inference_mode():\n",
    "# outputs = model.generate(input_ids=input_ids, max_new_tokens=512, do_sample=True, top_p=0.9)\n",
    "print(f\"input question: {sample['question']}\\n{'---'* 20}\")\n",
    "\n",
    "output = model.generate(input_ids=input_ids, max_new_tokens=256, output_hidden_states=True, output_scores=True,return_dict_in_generate=True, do_sample=True, top_p=0.9)\n",
    "#print(f\"----------{output}\")\n",
    "#ÂÖ®ÈÉ®ÁöÑËæìÂá∫\n",
    "decoded_output = [tokenizer.decode(ids) for ids in output.sequences]\n",
    "print(decoded_output)\n",
    "#####################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe39a1-e3ef-4652-84ad-1a28a9ce6275",
   "metadata": {},
   "source": [
    "# <center><font color=Green size=20>Evaluate (Metrics)</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb60a984-342d-4f01-84f8-20610581cea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10110/10110 [10:32:25<00:00,  3.75s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue1: 15.062190%\n",
      "rouge2: 2.108241%\n",
      "rougeL: 11.833166%\n",
      "rougeLsum: 11.831221%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6122b94221c945bfae24bc44adaf8751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112620184818903, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/Task-1/wandb/run-20240614_030347-ckpsqjjb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb' target=\"_blank\">train_1</a></strong> to <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca48dd99a78740d287cf6f0feaa7fa85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_Rogue1</td><td>‚ñÅ</td></tr><tr><td>test_Rogue2</td><td>‚ñÅ</td></tr><tr><td>test_RogueL</td><td>‚ñÅ</td></tr><tr><td>test_RogueLsum</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>logging_step</td><td>50400</td></tr><tr><td>test_Rogue1</td><td>0.15062</td></tr><tr><td>test_Rogue2</td><td>0.02108</td></tr><tr><td>test_RogueL</td><td>0.11833</td></tr><tr><td>test_RogueLsum</td><td>0.11831</td></tr><tr><td>train_loss</td><td>3.0688</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train_1</strong> at: <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb</a><br/>Synced 2 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240614_030347-ckpsqjjb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"./evaluate-main/metrics/rouge\")\n",
    "\n",
    "'''========================================================Evaluate PEFT model==============================================================='''\n",
    "def evaluate_peft_model(sample,max_target_length=512):\n",
    "    # generate summary\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    # decode eval sample\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels\n",
    "''' ========================================================================================================================================='''\n",
    "# load test dataset from disk\n",
    "test_dataset = load_from_disk(\"./Data/test/\").with_format(\"torch\")\n",
    "examples = test_dataset.select(range(5))\n",
    "\n",
    "# run predictions\n",
    "# this can take ~45 minutes\n",
    "predictions, references = [] , []\n",
    "\n",
    "# for sample in tqdm(examples):\n",
    "for sample in tqdm(test_dataset):\n",
    "    #print(sample.keys())\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "    predictions.append(p)\n",
    "    references.append(l)\n",
    "\n",
    "# compute metric \n",
    "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "# print results \n",
    "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
    "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
    "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
    "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n",
    "\n",
    "# write to wandb log\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "run = api.run(\"llm_learner/LLM_LoRA_FineTuning/ckpsqjjb\")\n",
    "run.config[\"test_Rogue1\"] = rogue['rouge1']\n",
    "run.config[\"test_Rogue2\"] = rogue['rouge2']\n",
    "run.config[\"test_RogueL\"] = rogue['rougeL']\n",
    "run.config[\"test_RogueLsum\"] = rogue['rougeLsum']\n",
    "run.update()\n",
    "\n",
    "project_name = \"LLM_LoRA_FineTuning\"\n",
    "run_id = \"ckpsqjjb\"  # ÂèØ‰ª•ÊòØ‰∏Ä‰∏™ÂîØ‰∏ÄÊ†áËØÜÁ¨¶ÔºåÂ¶ÇÊûúÂ∑≤ÁªèÊúâËøêË°å ID\n",
    "\n",
    "wandb.init(project=project_name, id=run_id, resume=\"allow\")\n",
    "wandb.log({\"test_Rogue1\": rogue['rouge1'], \"test_Rogue2\": rogue['rouge2'], \"test_RogueL\": rogue['rougeL'], \"test_RogueLsum\": rogue['rougeLsum']})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c2009-f36a-49e6-9009-2ad43af01cfc",
   "metadata": {},
   "source": [
    "# Ëé∑ÂèñBLEUËØÑ‰º∞ÊåáÊ†á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7554ec15-013f-4f59-8a26-dba9b754f52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10110/10110 [8:53:08<00:00,  3.16s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.008779300230546462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3df7a5bc9d8451fab538eafea582fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111256130453613, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/Task-1/wandb/run-20240614_140245-ckpsqjjb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb' target=\"_blank\">train_1</a></strong> to <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92f753eccf0455ab3431090d39af523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_BLEU</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>logging_step</td><td>50400</td></tr><tr><td>test_BLEU</td><td>0.00878</td></tr><tr><td>test_Rogue1</td><td>0.15062</td></tr><tr><td>test_Rogue2</td><td>0.02108</td></tr><tr><td>test_RogueL</td><td>0.11833</td></tr><tr><td>test_RogueLsum</td><td>0.11831</td></tr><tr><td>train_loss</td><td>3.0688</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train_1</strong> at: <a href='https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb' target=\"_blank\">https://wandb.ai/llm_learner/LLM_LoRA_FineTuning/runs/ckpsqjjb</a><br/>Synced 2 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240614_140245-ckpsqjjb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu_metric = evaluate.load(\"./evaluate-main/metrics/bleu\")\n",
    "# ËØÑ‰º∞ÁîüÊàêÁªìÊûú\n",
    "'''========================================================Evaluate PEFT model==============================================================='''\n",
    "def evaluate_peft_model(sample,max_target_length=512):\n",
    "    # generate summary\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    # decode eval sample\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels\n",
    "''' ========================================================================================================================================='''\n",
    "# load test dataset from disk\n",
    "test_dataset = load_from_disk(\"./Data/test/\").with_format(\"torch\")\n",
    "examples = test_dataset.select(range(50))\n",
    "\n",
    "# run predictions\n",
    "# this can take ~45 minutes\n",
    "predictions, references = [] , []\n",
    "\n",
    "#for sample in tqdm(examples):\n",
    "for sample in tqdm(test_dataset):\n",
    "    #print(sample.keys())\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "    predictions.append(p)\n",
    "    references.append(l)\n",
    "    \n",
    "results = bleu_metric.compute(predictions=predictions, references=references)\n",
    "print(f\"BLEU score: {results['bleu']}\")\n",
    "\n",
    "# write to wandb log\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "run = api.run(\"llm_learner/LLM_LoRA_FineTuning/ckpsqjjb\")\n",
    "run.config[\"test_BLEU\"] = results['bleu']\n",
    "run.update()\n",
    "\n",
    "project_name = \"LLM_LoRA_FineTuning\"\n",
    "run_id = \"ckpsqjjb\"  # ÂèØ‰ª•ÊòØ‰∏Ä‰∏™ÂîØ‰∏ÄÊ†áËØÜÁ¨¶ÔºåÂ¶ÇÊûúÂ∑≤ÁªèÊúâËøêË°å ID\n",
    "\n",
    "wandb.init(project=project_name, id=run_id, resume=\"allow\")\n",
    "wandb.log({\"test_BLEU\": results['bleu']})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a06b7c-f3e6-48a5-8453-99d5a2b41c9a",
   "metadata": {},
   "source": [
    "# 1-of-100 ranking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "144689ba-5770-42d2-9109-6e58fc8e92a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [20:01<00:00, 400.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metric for Recall@k\n",
    "metric = evaluate.load(\"./evaluate-main/metrics/recall\")\n",
    "\n",
    "'''========================================================Evaluate PEFT model==============================================================='''\n",
    "def evaluate_peft_model(sample, max_candidates=100, max_target_length=512):\n",
    "    # Generate summary for the input\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    \n",
    "    # Generate summaries for candidates\n",
    "    candidate_predictions = []\n",
    "    for _ in range(max_candidates):\n",
    "        candidate_output = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)\n",
    "        candidate_prediction = tokenizer.decode(candidate_output[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        candidate_predictions.append(candidate_prediction)\n",
    "    \n",
    "    return prediction, candidate_predictions\n",
    "\n",
    "''' ========================================================================================================================================='''\n",
    "# Load test dataset from disk\n",
    "test_dataset = load_from_disk(\"./Data/test/\").with_format(\"torch\")\n",
    "examples = test_dataset.select(range(3))\n",
    "\n",
    "# Run predictions\n",
    "predictions, references = [], []\n",
    "recall_at_k = 1  # You can set k as needed\n",
    "\n",
    "for sample in tqdm(examples):\n",
    "    # Generate 100 candidate answers for each sample\n",
    "    prediction, candidate_predictions = evaluate_peft_model(sample, max_candidates=100)\n",
    "    \n",
    "    # Get the ground truth answer\n",
    "    ground_truth = tokenizer.decode(np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id), skip_special_tokens=True)\n",
    "    \n",
    "    # Check if ground truth is in the top k predictions\n",
    "    if ground_truth in candidate_predictions[:recall_at_k]:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "    references.append(1)\n",
    "\n",
    "# Compute metric \n",
    "recall_results = metric.compute(predictions=predictions, references=references)\n",
    "recall_at_k_value = recall_results['recall']\n",
    "\n",
    "# Print results \n",
    "print(f\"Recall@{recall_at_k}: {recall_at_k_value * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce280dbc-d52a-45a4-b25e-ccf4e84ee744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
