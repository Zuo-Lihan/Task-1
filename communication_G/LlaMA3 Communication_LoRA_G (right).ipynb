{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb2e8a0-c402-4233-bb72-687c65051ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad6ecd-4102-43a6-a6ba-b5ae6f38789b",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf53072d-5b40-4c2e-b1e1-07f23494508d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a5ea6b91ab48b2917ab1b0d6677a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/root/autodl-fs/Llama-3-12B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "model = model.cuda()\n",
    "\n",
    "# model.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n",
    "\n",
    "model.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f24ef4-35a3-4643-819d-0a91f86c6f70",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1c219d-6710-43a2-b2d1-bc0460c33632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': [1, 2, 3],\n",
       " 'conv': ['hello',\n",
       "  'hello there, I have not seen this movie so im going to take a minute to look it over :)',\n",
       "  'Alright that is fine. What is the movie?'],\n",
       " 'response': ['hello there, I have not seen this movie so im going to take a minute to look it over :)',\n",
       "  'Alright that is fine. What is the movie?',\n",
       "  'The movie is The Social Network']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file = \"pairs/train_pairs.json\"\n",
    "df_train = pd.read_json(dataset_file)\n",
    "\n",
    "'''---------------------'''\n",
    "df_train = df_train[:20000]\n",
    "'''---------------------'''\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_train)\n",
    "\n",
    "ds_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8434401-a447-4f94-95f6-1c8ed72b6fec",
   "metadata": {},
   "source": [
    "# Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "208f07b7-9b0e-481b-9e12-116f8bef4ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c35dae3-570c-48af-a2f1-b617c67fcadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|end_of_text|>', 128001, 128001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6a6b70-1ba9-466e-b0aa-2b7c66ef9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 66  # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(\n",
    "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n{example['conv']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['response']}<|eot_id|>\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ec69ce6-d5d0-4b60-a204-9a24147403d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280a5a21f8304acd8a0f894e4b9666bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized_id = ds_train.map(process_func, remove_columns=ds_train.column_names)\n",
    "train_tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea5e1b5-e008-466a-9176-d578f28c6ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "hello there, I have not seen this movie so im going to take a minute to look it over :)<|eot_id|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_tokenized_id[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02cf4a74-3219-4338-b510-805c05ab4b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright that is fine. What is the movie?<|eot_id|><|end_of_text|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, train_tokenized_id[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964fecb6-a9d9-4bdd-acf8-98aa8efbbb73",
   "metadata": {},
   "source": [
    "# Config of LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf3e6899-f5fe-4a4b-bfa7-e57dd62814fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: trainable params: 31,457,280 || all params: 11,551,510,528 || trainable%: 0.27232178790600503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training,TaskType\n",
    "import wandb\n",
    "config1 = {\"r\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"inference_mode\":False,  # train mode\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": TaskType.CAUSAL_LM\n",
    "         }\n",
    "\n",
    "\n",
    "# define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r = config1[\"r\"],\n",
    "    lora_alpha = config1[\"lora_alpha\"],\n",
    "    target_modules = config1[\"target_modules\"],\n",
    "    lora_dropout = config1[\"lora_dropout\"],\n",
    "    inference_mode = config1[\"inference_mode\"],\n",
    "    bias = config1[\"bias\"],\n",
    "    task_type = config1[\"task_type\"]\n",
    ")\n",
    "\n",
    "\n",
    "# # prepare int8 model for training\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adapter\n",
    "model = get_peft_model(model, lora_config)\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# 假设 model 是您已经定义好的模型对象\n",
    "\n",
    "# 创建一个 StringIO 对象\n",
    "output = io.StringIO()\n",
    "\n",
    "# 保存当前的 stdout\n",
    "original_stdout = sys.stdout\n",
    "\n",
    "try:\n",
    "    # 将 stdout 重定向到 StringIO 对象\n",
    "    sys.stdout = output\n",
    "    # 调用方法，打印输出到 StringIO 对象\n",
    "    model.print_trainable_parameters()\n",
    "finally:\n",
    "    # 恢复原始的 stdout\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "# 获取 StringIO 对象中的内容\n",
    "output_str = output.getvalue()\n",
    "\n",
    "# 关闭 StringIO 对象\n",
    "output.close()\n",
    "\n",
    "# 打印捕获到的字符串内容\n",
    "print(\"1:\",output_str)\n",
    "\n",
    "# wandb.init(\n",
    "#     project = \"LLM_LoRA_FineTuning\",\n",
    "    \n",
    "#     config = {\n",
    "#         \"config\": config1,\n",
    "#         \"Dataset\": \"qa_Tools_and_Home_Improvement\",\n",
    "#         \"Tuning-method\": \"LoRA\",\n",
    "#         \"Trainable params\": str(output_str)\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab90cd-8c12-4b46-bc90-54239e330c34",
   "metadata": {},
   "source": [
    "# Config of Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98df6196-907b-42cf-bb9f-357e8b8209d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100 \n",
    "# 在训练序列到序列（Seq2Seq）模型时，通常会使用特殊的填充标记来对输入进行处理。对于标签数据，在计算损失函数时，我们需要忽略填充标记所带来的影响，因为这些填充部分不应该参与到损失的计算中。\n",
    "# 在 Transformers 库中，通常将不应该被考虑的标签设置为一个特定的值，通常是 -100。当计算损失函数时，模型会忽略这些 -100 值所对应的预测结果，只计算真实标签部分的损失值，从而实现在计算损失函数时忽略填充标记的效果。 \n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "771cfb02-dd6a-47f4-a4ba-4284b618084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "class TrainLogCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if 'loss' in state.log_history[-1]:\n",
    "            self.train_losses.append(state.log_history[-1]['loss'])\n",
    "            if state.global_step % args.logging_steps == 0:\n",
    "                print(f\"Logging step {state.global_step} at epoch {state.epoch}, train_loss: {state.log_history[-1]['loss']}\")\n",
    "                wandb.log({\"logging_step\": state.global_step, \"train_loss\": state.log_history[-1]['loss']})\n",
    "            # print(f\"Step: {state.global_step}, Train Loss: {state.log_history[-1]['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd32fb82-a341-486e-8aee-6a80ed2f3909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzuolihanstudy\u001b[0m (\u001b[33mllm_learner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9bd5b7b9834e0589aab810424f7728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112723996241887, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/Task-1/Conversation_G/wandb/run-20240615_174752-4wse90al</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llm_learner/Llama3_Conversation/runs/4wse90al' target=\"_blank\">charmed-eon-23</a></strong> to <a href='https://wandb.ai/llm_learner/Llama3_Conversation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm_learner/Llama3_Conversation' target=\"_blank\">https://wandb.ai/llm_learner/Llama3_Conversation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm_learner/Llama3_Conversation/runs/4wse90al' target=\"_blank\">https://wandb.ai/llm_learner/Llama3_Conversation/runs/4wse90al</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "output_dir=\"llama-3\"\n",
    "\n",
    "config2 = {\n",
    "    \"output_dir\": output_dir,\n",
    "    \"auto_find_batch_size\": True,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"num_train_epochs\":5,\n",
    "    \"logging_dir\":f\"{output_dir}/logs\",\n",
    "    \"logging_strategy\":\"steps\",\n",
    "    \"logging_steps\":50,\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"report_to\": \"tensorboard\"\n",
    "}\n",
    "\n",
    "# 初始化自定义回调\n",
    "log_step_callback = TrainLogCallback()\n",
    "\n",
    "# Define training args\n",
    "training_args =TrainingArguments(\n",
    "    output_dir = config2[\"output_dir\"],\n",
    "    auto_find_batch_size = config2[\"auto_find_batch_size\"],\n",
    "    # per_device_train_batch_size = config2['per_device_train_batch_size'],\n",
    "    gradient_accumulation_steps = config2['gradient_accumulation_steps'],\n",
    "    learning_rate = config2[\"learning_rate\"], # higher learning rate\n",
    "    num_train_epochs = config2[\"num_train_epochs\"],\n",
    "    logging_dir = config2[\"logging_dir\"],\n",
    "    logging_strategy = config2[\"logging_strategy\"],\n",
    "    logging_steps = config2[\"logging_steps\"],\n",
    "    save_strategy = config2[\"save_strategy\"],\n",
    "    report_to = config2[\"report_to\"],\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project = \"Llama3_Conversation\",\n",
    "    \n",
    "    config = {\n",
    "        \"config1\": config1,\n",
    "        \"config2\": config2,\n",
    "        \"Dataset\": \"CMU_DoG\",\n",
    "        \"Tuning-method\": \"LoRA\",\n",
    "        \"Trainable params\": str(output_str)\n",
    "    }\n",
    ")\n",
    "\n",
    "# api = wandb.Api()\n",
    "\n",
    "# run = api.run(\"llm_learner/LLM_LoRA_FineTuning/623waqcp\")\n",
    "# run.config[\"config2\"] = config2\n",
    "# run.update()\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_tokenized_id,\n",
    "    callbacks=[log_step_callback]\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98092eea-1454-41a5-a345-5be5c1cdee30",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c27143b-a20c-407f-9a47-4164bf33b49b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 6:10:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.675100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.820900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.928800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.927400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.785800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.778100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.642300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.653800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.852200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.776600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.694800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.772300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.706700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.595800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.507100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.632400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.525100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.611600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.773500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.371600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.515700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>3.776900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>3.515500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.484900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>3.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.579800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>3.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.657400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>3.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>3.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.403500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>3.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>3.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.612700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>3.589300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>3.419300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>3.420500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>3.506600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>3.551500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>3.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.553200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>3.425500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.498600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>3.638900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.564100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>3.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>3.412900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.547800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>3.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.489500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>3.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>3.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>3.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>3.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>3.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>3.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>3.476500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>3.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>3.474800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.554900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>3.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>3.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>3.453500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>3.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>3.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>3.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.512700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>3.436500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>3.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>3.413900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>3.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>3.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>3.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>3.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>3.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>3.365900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>3.404100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>3.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.358500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>3.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>3.564700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>3.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>3.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>3.337200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>3.482300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>3.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>3.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>3.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.676300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>3.462300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.418800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>3.444900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.412700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>3.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>3.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>3.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>3.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>3.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>3.410200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>3.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>3.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>3.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.509900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>3.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>3.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>3.394600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>3.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>3.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>3.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>3.567800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>3.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>3.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>3.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>3.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>3.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>3.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>3.420500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>3.433300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>3.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>3.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>3.352200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>3.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>3.326700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.383700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>3.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>3.558700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>3.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>3.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>3.515100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>3.543600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>3.526300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>3.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>3.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.440200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>3.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>3.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>3.301400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>3.352700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>3.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>3.371100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>3.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>3.340400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>3.342300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.238500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>3.328500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>3.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>3.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>3.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>3.603900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>3.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>3.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>3.465300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>3.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>3.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>3.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>3.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>3.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>3.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>3.432600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>3.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>3.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>3.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>3.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>3.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>3.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>3.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>3.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>3.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>3.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>3.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>3.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>3.504900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>3.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>3.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>3.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>3.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>3.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>3.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>3.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>3.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>3.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>3.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>3.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>3.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>3.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>3.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>3.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>3.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>3.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>3.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>3.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>3.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>3.384100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>3.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>3.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>3.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>3.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.380800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>3.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>3.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>3.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>3.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>3.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>3.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>3.379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>3.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>3.406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.284100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>3.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>3.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>3.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>3.268800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>3.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>3.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>3.334100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>3.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>3.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>3.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>3.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>3.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>3.536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>3.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>3.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>3.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>3.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>3.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>3.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>3.320200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>3.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>3.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>3.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>3.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>3.329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>3.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>3.480700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.308400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>3.453100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>3.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14650</td>\n",
       "      <td>3.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>3.677600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>3.281500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>3.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14850</td>\n",
       "      <td>3.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>3.283700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14950</td>\n",
       "      <td>3.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.318900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15050</td>\n",
       "      <td>3.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>3.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15150</td>\n",
       "      <td>3.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>3.471500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>3.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>3.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15350</td>\n",
       "      <td>3.457900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>3.203400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15450</td>\n",
       "      <td>3.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15550</td>\n",
       "      <td>3.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>3.285100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15650</td>\n",
       "      <td>3.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>3.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>3.407100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>3.565400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15850</td>\n",
       "      <td>3.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>3.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15950</td>\n",
       "      <td>3.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16050</td>\n",
       "      <td>3.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>3.356600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16150</td>\n",
       "      <td>3.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>3.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>3.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>3.532200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16350</td>\n",
       "      <td>3.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>3.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16450</td>\n",
       "      <td>3.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16550</td>\n",
       "      <td>3.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>3.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16650</td>\n",
       "      <td>3.298600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>3.408700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16750</td>\n",
       "      <td>3.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>3.329200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16850</td>\n",
       "      <td>3.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>3.305500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16950</td>\n",
       "      <td>3.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.341200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17050</td>\n",
       "      <td>3.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>3.432300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17150</td>\n",
       "      <td>3.352200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>3.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17250</td>\n",
       "      <td>3.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>3.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17350</td>\n",
       "      <td>3.440600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>3.387800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17450</td>\n",
       "      <td>3.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17550</td>\n",
       "      <td>3.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>3.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17650</td>\n",
       "      <td>3.393400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>3.416300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17750</td>\n",
       "      <td>3.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>3.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17850</td>\n",
       "      <td>3.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>3.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17950</td>\n",
       "      <td>3.380200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18050</td>\n",
       "      <td>3.399500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>3.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18150</td>\n",
       "      <td>3.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>3.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18250</td>\n",
       "      <td>3.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>3.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18350</td>\n",
       "      <td>3.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>3.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18450</td>\n",
       "      <td>3.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.254600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18550</td>\n",
       "      <td>3.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>3.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18650</td>\n",
       "      <td>3.325900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>3.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>3.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>3.260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18850</td>\n",
       "      <td>3.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>3.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18950</td>\n",
       "      <td>3.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19050</td>\n",
       "      <td>3.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>3.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19150</td>\n",
       "      <td>3.274900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>3.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19250</td>\n",
       "      <td>3.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>3.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19350</td>\n",
       "      <td>3.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>3.473100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19450</td>\n",
       "      <td>3.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19550</td>\n",
       "      <td>3.315400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.396500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19650</td>\n",
       "      <td>3.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>3.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19750</td>\n",
       "      <td>3.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>3.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19850</td>\n",
       "      <td>3.252900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>3.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19950</td>\n",
       "      <td>3.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20050</td>\n",
       "      <td>3.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>3.329200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20150</td>\n",
       "      <td>3.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>3.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20250</td>\n",
       "      <td>3.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>3.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20350</td>\n",
       "      <td>3.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>3.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20450</td>\n",
       "      <td>3.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20550</td>\n",
       "      <td>3.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>3.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20650</td>\n",
       "      <td>3.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>3.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20750</td>\n",
       "      <td>3.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>3.370400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20850</td>\n",
       "      <td>3.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>3.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20950</td>\n",
       "      <td>3.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21050</td>\n",
       "      <td>3.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>3.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21150</td>\n",
       "      <td>3.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>3.344800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21250</td>\n",
       "      <td>3.374400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>3.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21350</td>\n",
       "      <td>3.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>3.413900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21450</td>\n",
       "      <td>3.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21550</td>\n",
       "      <td>3.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>3.346900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21650</td>\n",
       "      <td>3.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>3.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21750</td>\n",
       "      <td>3.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>3.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21850</td>\n",
       "      <td>3.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>3.348600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21950</td>\n",
       "      <td>3.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22050</td>\n",
       "      <td>3.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>3.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22150</td>\n",
       "      <td>3.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>3.387300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22250</td>\n",
       "      <td>3.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>3.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22350</td>\n",
       "      <td>3.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>3.321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22450</td>\n",
       "      <td>3.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22550</td>\n",
       "      <td>3.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>3.503300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22650</td>\n",
       "      <td>3.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>3.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22750</td>\n",
       "      <td>3.331100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>3.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22850</td>\n",
       "      <td>3.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>3.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22950</td>\n",
       "      <td>3.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23050</td>\n",
       "      <td>3.278300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>3.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23150</td>\n",
       "      <td>3.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>3.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23250</td>\n",
       "      <td>3.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>3.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23350</td>\n",
       "      <td>3.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>3.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23450</td>\n",
       "      <td>3.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23550</td>\n",
       "      <td>3.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>3.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23650</td>\n",
       "      <td>3.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>3.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23750</td>\n",
       "      <td>3.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>3.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23850</td>\n",
       "      <td>3.318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>3.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23950</td>\n",
       "      <td>3.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.321300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24050</td>\n",
       "      <td>3.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>3.427500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24150</td>\n",
       "      <td>3.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>3.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24250</td>\n",
       "      <td>3.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>3.353200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24350</td>\n",
       "      <td>3.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>3.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24450</td>\n",
       "      <td>3.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24550</td>\n",
       "      <td>3.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>3.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24650</td>\n",
       "      <td>3.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>3.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24750</td>\n",
       "      <td>3.269500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>3.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24850</td>\n",
       "      <td>3.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>3.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24950</td>\n",
       "      <td>3.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.303600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging step 50 at epoch 0.01, train_loss: 6.6751\n",
      "Logging step 100 at epoch 0.02, train_loss: 5.3925\n",
      "Logging step 150 at epoch 0.03, train_loss: 4.7017\n",
      "Logging step 200 at epoch 0.04, train_loss: 4.6044\n",
      "Logging step 250 at epoch 0.05, train_loss: 4.338\n",
      "Logging step 300 at epoch 0.06, train_loss: 3.9936\n",
      "Logging step 350 at epoch 0.07, train_loss: 4.0023\n",
      "Logging step 400 at epoch 0.08, train_loss: 3.8209\n",
      "Logging step 450 at epoch 0.09, train_loss: 3.9288\n",
      "Logging step 500 at epoch 0.1, train_loss: 3.9274\n",
      "Logging step 550 at epoch 0.11, train_loss: 3.9422\n",
      "Logging step 600 at epoch 0.12, train_loss: 3.7858\n",
      "Logging step 650 at epoch 0.13, train_loss: 3.7781\n",
      "Logging step 700 at epoch 0.14, train_loss: 3.6423\n",
      "Logging step 750 at epoch 0.15, train_loss: 3.7174\n",
      "Logging step 800 at epoch 0.16, train_loss: 3.6538\n",
      "Logging step 850 at epoch 0.17, train_loss: 3.6016\n",
      "Logging step 900 at epoch 0.18, train_loss: 3.8522\n",
      "Logging step 950 at epoch 0.19, train_loss: 3.7766\n",
      "Logging step 1000 at epoch 0.2, train_loss: 3.6948\n",
      "Logging step 1050 at epoch 0.21, train_loss: 3.7723\n",
      "Logging step 1100 at epoch 0.22, train_loss: 3.7067\n",
      "Logging step 1150 at epoch 0.23, train_loss: 3.6761\n",
      "Logging step 1200 at epoch 0.24, train_loss: 3.5958\n",
      "Logging step 1250 at epoch 0.25, train_loss: 3.6016\n",
      "Logging step 1300 at epoch 0.26, train_loss: 3.5071\n",
      "Logging step 1350 at epoch 0.27, train_loss: 3.6324\n",
      "Logging step 1400 at epoch 0.28, train_loss: 3.5251\n",
      "Logging step 1450 at epoch 0.29, train_loss: 3.6806\n",
      "Logging step 1500 at epoch 0.3, train_loss: 3.6038\n",
      "Logging step 1550 at epoch 0.31, train_loss: 3.5085\n",
      "Logging step 1600 at epoch 0.32, train_loss: 3.6116\n",
      "Logging step 1650 at epoch 0.33, train_loss: 3.6461\n",
      "Logging step 1700 at epoch 0.34, train_loss: 3.7735\n",
      "Logging step 1750 at epoch 0.35, train_loss: 3.3716\n",
      "Logging step 1800 at epoch 0.36, train_loss: 3.3589\n",
      "Logging step 1850 at epoch 0.37, train_loss: 3.7994\n",
      "Logging step 1900 at epoch 0.38, train_loss: 3.6406\n",
      "Logging step 1950 at epoch 0.39, train_loss: 3.5157\n",
      "Logging step 2000 at epoch 0.4, train_loss: 3.5565\n",
      "Logging step 2050 at epoch 0.41, train_loss: 3.7769\n",
      "Logging step 2100 at epoch 0.42, train_loss: 3.6531\n",
      "Logging step 2150 at epoch 0.43, train_loss: 3.5155\n",
      "Logging step 2200 at epoch 0.44, train_loss: 3.4849\n",
      "Logging step 2250 at epoch 0.45, train_loss: 3.5741\n",
      "Logging step 2300 at epoch 0.46, train_loss: 3.5798\n",
      "Logging step 2350 at epoch 0.47, train_loss: 3.4626\n",
      "Logging step 2400 at epoch 0.48, train_loss: 3.6574\n",
      "Logging step 2450 at epoch 0.49, train_loss: 3.6049\n",
      "Logging step 2500 at epoch 0.5, train_loss: 3.8148\n",
      "Logging step 2550 at epoch 0.51, train_loss: 3.4721\n",
      "Logging step 2600 at epoch 0.52, train_loss: 3.4035\n",
      "Logging step 2650 at epoch 0.53, train_loss: 3.4101\n",
      "Logging step 2700 at epoch 0.54, train_loss: 3.6375\n",
      "Logging step 2750 at epoch 0.55, train_loss: 3.5895\n",
      "Logging step 2800 at epoch 0.56, train_loss: 3.6127\n",
      "Logging step 2850 at epoch 0.57, train_loss: 3.5893\n",
      "Logging step 2900 at epoch 0.58, train_loss: 3.4193\n",
      "Logging step 2950 at epoch 0.59, train_loss: 3.4205\n",
      "Logging step 3000 at epoch 0.6, train_loss: 3.5125\n",
      "Logging step 3050 at epoch 0.61, train_loss: 3.5066\n",
      "Logging step 3100 at epoch 0.62, train_loss: 3.5955\n",
      "Logging step 3150 at epoch 0.63, train_loss: 3.5515\n",
      "Logging step 3200 at epoch 0.64, train_loss: 3.4068\n",
      "Logging step 3250 at epoch 0.65, train_loss: 3.6652\n",
      "Logging step 3300 at epoch 0.66, train_loss: 3.5532\n",
      "Logging step 3350 at epoch 0.67, train_loss: 3.4255\n",
      "Logging step 3400 at epoch 0.68, train_loss: 3.4986\n",
      "Logging step 3450 at epoch 0.69, train_loss: 3.6389\n",
      "Logging step 3500 at epoch 0.7, train_loss: 3.5641\n",
      "Logging step 3550 at epoch 0.71, train_loss: 3.7594\n",
      "Logging step 3600 at epoch 0.72, train_loss: 3.3358\n",
      "Logging step 3650 at epoch 0.73, train_loss: 3.4129\n",
      "Logging step 3700 at epoch 0.74, train_loss: 3.5478\n",
      "Logging step 3750 at epoch 0.75, train_loss: 3.5759\n",
      "Logging step 3800 at epoch 0.76, train_loss: 3.4895\n",
      "Logging step 3850 at epoch 0.77, train_loss: 3.4088\n",
      "Logging step 3900 at epoch 0.78, train_loss: 3.4736\n",
      "Logging step 3950 at epoch 0.79, train_loss: 3.458\n",
      "Logging step 4000 at epoch 0.8, train_loss: 3.4262\n",
      "Logging step 4050 at epoch 0.81, train_loss: 3.4141\n",
      "Logging step 4100 at epoch 0.82, train_loss: 3.401\n",
      "Logging step 4150 at epoch 0.83, train_loss: 3.5851\n",
      "Logging step 4200 at epoch 0.84, train_loss: 3.4712\n",
      "Logging step 4250 at epoch 0.85, train_loss: 3.466\n",
      "Logging step 4300 at epoch 0.86, train_loss: 3.4765\n",
      "Logging step 4350 at epoch 0.87, train_loss: 3.4568\n",
      "Logging step 4400 at epoch 0.88, train_loss: 3.3889\n",
      "Logging step 4450 at epoch 0.89, train_loss: 3.4748\n",
      "Logging step 4500 at epoch 0.9, train_loss: 3.5549\n",
      "Logging step 4550 at epoch 0.91, train_loss: 3.438\n",
      "Logging step 4600 at epoch 0.92, train_loss: 3.4914\n",
      "Logging step 4650 at epoch 0.93, train_loss: 3.4248\n",
      "Logging step 4700 at epoch 0.94, train_loss: 3.4136\n",
      "Logging step 4750 at epoch 0.95, train_loss: 3.4535\n",
      "Logging step 4800 at epoch 0.96, train_loss: 3.5744\n",
      "Logging step 4850 at epoch 0.97, train_loss: 3.3662\n",
      "Logging step 4900 at epoch 0.98, train_loss: 3.4433\n",
      "Logging step 4950 at epoch 0.99, train_loss: 3.615\n",
      "Logging step 5000 at epoch 1.0, train_loss: 3.5127\n",
      "Logging step 5050 at epoch 1.01, train_loss: 3.4365\n",
      "Logging step 5100 at epoch 1.02, train_loss: 3.3462\n",
      "Logging step 5150 at epoch 1.03, train_loss: 3.4139\n",
      "Logging step 5200 at epoch 1.04, train_loss: 3.5008\n",
      "Logging step 5250 at epoch 1.05, train_loss: 3.4943\n",
      "Logging step 5300 at epoch 1.06, train_loss: 3.7006\n",
      "Logging step 5350 at epoch 1.07, train_loss: 3.3571\n",
      "Logging step 5400 at epoch 1.08, train_loss: 3.2054\n",
      "Logging step 5450 at epoch 1.09, train_loss: 3.3936\n",
      "Logging step 5500 at epoch 1.1, train_loss: 3.1886\n",
      "Logging step 5550 at epoch 1.11, train_loss: 3.471\n",
      "Logging step 5600 at epoch 1.12, train_loss: 3.4036\n",
      "Logging step 5650 at epoch 1.13, train_loss: 3.3659\n",
      "Logging step 5700 at epoch 1.1400000000000001, train_loss: 3.4041\n",
      "Logging step 5750 at epoch 1.15, train_loss: 3.297\n",
      "Logging step 5800 at epoch 1.16, train_loss: 3.3585\n",
      "Logging step 5850 at epoch 1.17, train_loss: 3.3912\n",
      "Logging step 5900 at epoch 1.18, train_loss: 3.5647\n",
      "Logging step 5950 at epoch 1.19, train_loss: 3.4772\n",
      "Logging step 6000 at epoch 1.2, train_loss: 3.3437\n",
      "Logging step 6050 at epoch 1.21, train_loss: 3.4016\n",
      "Logging step 6100 at epoch 1.22, train_loss: 3.3372\n",
      "Logging step 6150 at epoch 1.23, train_loss: 3.4823\n",
      "Logging step 6200 at epoch 1.24, train_loss: 3.4297\n",
      "Logging step 6250 at epoch 1.25, train_loss: 3.246\n",
      "Logging step 6300 at epoch 1.26, train_loss: 3.3911\n",
      "Logging step 6350 at epoch 1.27, train_loss: 3.3928\n",
      "Logging step 6400 at epoch 1.28, train_loss: 3.6763\n",
      "Logging step 6450 at epoch 1.29, train_loss: 3.4623\n",
      "Logging step 6500 at epoch 1.3, train_loss: 3.4188\n",
      "Logging step 6550 at epoch 1.31, train_loss: 3.4449\n",
      "Logging step 6600 at epoch 1.32, train_loss: 3.4127\n",
      "Logging step 6650 at epoch 1.33, train_loss: 3.5004\n",
      "Logging step 6700 at epoch 1.34, train_loss: 3.3562\n",
      "Logging step 6750 at epoch 1.35, train_loss: 3.5495\n",
      "Logging step 6800 at epoch 1.3599999999999999, train_loss: 3.2035\n",
      "Logging step 6850 at epoch 1.37, train_loss: 3.5064\n",
      "Logging step 6900 at epoch 1.38, train_loss: 3.5495\n",
      "Logging step 6950 at epoch 1.3900000000000001, train_loss: 3.4102\n",
      "Logging step 7000 at epoch 1.4, train_loss: 3.4712\n",
      "Logging step 7050 at epoch 1.41, train_loss: 3.4012\n",
      "Logging step 7100 at epoch 1.42, train_loss: 3.427\n",
      "Logging step 7150 at epoch 1.43, train_loss: 3.3261\n",
      "Logging step 7200 at epoch 1.44, train_loss: 3.5099\n",
      "Logging step 7250 at epoch 1.45, train_loss: 3.2812\n",
      "Logging step 7300 at epoch 1.46, train_loss: 3.371\n",
      "Logging step 7350 at epoch 1.47, train_loss: 3.3946\n",
      "Logging step 7400 at epoch 1.48, train_loss: 3.2238\n",
      "Logging step 7450 at epoch 1.49, train_loss: 3.3081\n",
      "Logging step 7500 at epoch 1.5, train_loss: 3.3969\n",
      "Logging step 7550 at epoch 1.51, train_loss: 3.272\n",
      "Logging step 7600 at epoch 1.52, train_loss: 3.44\n",
      "Logging step 7650 at epoch 1.53, train_loss: 3.5678\n",
      "Logging step 7700 at epoch 1.54, train_loss: 3.2998\n",
      "Logging step 7750 at epoch 1.55, train_loss: 3.3207\n",
      "Logging step 7800 at epoch 1.56, train_loss: 3.2895\n",
      "Logging step 7850 at epoch 1.5699999999999998, train_loss: 3.4958\n",
      "Logging step 7900 at epoch 1.58, train_loss: 3.359\n",
      "Logging step 7950 at epoch 1.5899999999999999, train_loss: 3.5633\n",
      "Logging step 8000 at epoch 1.6, train_loss: 3.356\n",
      "Logging step 8050 at epoch 1.6099999999999999, train_loss: 3.4205\n",
      "Logging step 8100 at epoch 1.62, train_loss: 3.4333\n",
      "Logging step 8150 at epoch 1.63, train_loss: 3.3502\n",
      "Logging step 8200 at epoch 1.6400000000000001, train_loss: 3.2681\n",
      "Logging step 8250 at epoch 1.65, train_loss: 3.3522\n",
      "Logging step 8300 at epoch 1.6600000000000001, train_loss: 3.3211\n",
      "Logging step 8350 at epoch 1.67, train_loss: 3.3267\n",
      "Logging step 8400 at epoch 1.6800000000000002, train_loss: 3.3837\n",
      "Logging step 8450 at epoch 1.69, train_loss: 3.356\n",
      "Logging step 8500 at epoch 1.7, train_loss: 3.2794\n",
      "Logging step 8550 at epoch 1.71, train_loss: 3.5587\n",
      "Logging step 8600 at epoch 1.72, train_loss: 3.2989\n",
      "Logging step 8650 at epoch 1.73, train_loss: 3.4912\n",
      "Logging step 8700 at epoch 1.74, train_loss: 3.2366\n",
      "Logging step 8750 at epoch 1.75, train_loss: 3.5151\n",
      "Logging step 8800 at epoch 1.76, train_loss: 3.5436\n",
      "Logging step 8850 at epoch 1.77, train_loss: 3.5263\n",
      "Logging step 8900 at epoch 1.78, train_loss: 3.4626\n",
      "Logging step 8950 at epoch 1.79, train_loss: 3.4725\n",
      "Logging step 9000 at epoch 1.8, train_loss: 3.4402\n",
      "Logging step 9050 at epoch 1.81, train_loss: 3.3836\n",
      "Logging step 9100 at epoch 1.8199999999999998, train_loss: 3.2322\n",
      "Logging step 9150 at epoch 1.83, train_loss: 3.3014\n",
      "Logging step 9200 at epoch 1.8399999999999999, train_loss: 3.3527\n",
      "Logging step 9250 at epoch 1.85, train_loss: 3.316\n",
      "Logging step 9300 at epoch 1.8599999999999999, train_loss: 3.3711\n",
      "Logging step 9350 at epoch 1.87, train_loss: 3.4994\n",
      "Logging step 9400 at epoch 1.88, train_loss: 3.3404\n",
      "Logging step 9450 at epoch 1.8900000000000001, train_loss: 3.3423\n",
      "Logging step 9500 at epoch 1.9, train_loss: 3.2385\n",
      "Logging step 9550 at epoch 1.9100000000000001, train_loss: 3.3285\n",
      "Logging step 9600 at epoch 1.92, train_loss: 3.3829\n",
      "Logging step 9650 at epoch 1.9300000000000002, train_loss: 3.3196\n",
      "Logging step 9700 at epoch 1.94, train_loss: 3.4221\n",
      "Logging step 9750 at epoch 1.95, train_loss: 3.4286\n",
      "Logging step 9800 at epoch 1.96, train_loss: 3.6039\n",
      "Logging step 9850 at epoch 1.97, train_loss: 3.2392\n",
      "Logging step 9900 at epoch 1.98, train_loss: 3.3969\n",
      "Logging step 9950 at epoch 1.99, train_loss: 3.4653\n",
      "Logging step 10000 at epoch 2.0, train_loss: 3.3343\n",
      "Logging step 10050 at epoch 2.01, train_loss: 3.3192\n",
      "Logging step 10100 at epoch 2.02, train_loss: 3.5177\n",
      "Logging step 10150 at epoch 2.03, train_loss: 3.3933\n",
      "Logging step 10200 at epoch 2.04, train_loss: 3.2635\n",
      "Logging step 10250 at epoch 2.05, train_loss: 3.2653\n",
      "Logging step 10300 at epoch 2.06, train_loss: 3.3366\n",
      "Logging step 10350 at epoch 2.07, train_loss: 3.4326\n",
      "Logging step 10400 at epoch 2.08, train_loss: 3.1771\n",
      "Logging step 10450 at epoch 2.09, train_loss: 3.2248\n",
      "Logging step 10500 at epoch 2.1, train_loss: 3.5203\n",
      "Logging step 10550 at epoch 2.11, train_loss: 3.392\n",
      "Logging step 10600 at epoch 2.12, train_loss: 3.3174\n",
      "Logging step 10650 at epoch 2.13, train_loss: 3.343\n",
      "Logging step 10700 at epoch 2.14, train_loss: 3.3057\n",
      "Logging step 10750 at epoch 2.15, train_loss: 3.3797\n",
      "Logging step 10800 at epoch 2.16, train_loss: 3.362\n",
      "Logging step 10850 at epoch 2.17, train_loss: 3.3759\n",
      "Logging step 10900 at epoch 2.18, train_loss: 3.3462\n",
      "Logging step 10950 at epoch 2.19, train_loss: 3.5167\n",
      "Logging step 11000 at epoch 2.2, train_loss: 3.3123\n",
      "Logging step 11050 at epoch 2.21, train_loss: 3.3732\n",
      "Logging step 11100 at epoch 2.22, train_loss: 3.5049\n",
      "Logging step 11150 at epoch 2.23, train_loss: 3.3811\n",
      "Logging step 11200 at epoch 2.24, train_loss: 3.3258\n",
      "Logging step 11250 at epoch 2.25, train_loss: 3.2752\n",
      "Logging step 11300 at epoch 2.26, train_loss: 3.3835\n",
      "Logging step 11350 at epoch 2.27, train_loss: 3.3405\n",
      "Logging step 11400 at epoch 2.2800000000000002, train_loss: 3.6048\n",
      "Logging step 11450 at epoch 2.29, train_loss: 3.4254\n",
      "Logging step 11500 at epoch 2.3, train_loss: 3.4547\n",
      "Logging step 11550 at epoch 2.31, train_loss: 3.3832\n",
      "Logging step 11600 at epoch 2.32, train_loss: 3.3418\n",
      "Logging step 11650 at epoch 2.33, train_loss: 3.2896\n",
      "Logging step 11700 at epoch 2.34, train_loss: 3.1442\n",
      "Logging step 11750 at epoch 2.35, train_loss: 3.2581\n",
      "Logging step 11800 at epoch 2.36, train_loss: 3.2395\n",
      "Logging step 11850 at epoch 2.37, train_loss: 3.3007\n",
      "Logging step 11900 at epoch 2.38, train_loss: 3.2809\n",
      "Logging step 11950 at epoch 2.39, train_loss: 3.4348\n",
      "Logging step 12000 at epoch 2.4, train_loss: 3.4015\n",
      "Logging step 12050 at epoch 2.41, train_loss: 3.3136\n",
      "Logging step 12100 at epoch 2.42, train_loss: 3.3253\n",
      "Logging step 12150 at epoch 2.43, train_loss: 3.3373\n",
      "Logging step 12200 at epoch 2.44, train_loss: 3.2579\n",
      "Logging step 12250 at epoch 2.45, train_loss: 3.3841\n",
      "Logging step 12300 at epoch 2.46, train_loss: 3.4155\n",
      "Logging step 12350 at epoch 2.4699999999999998, train_loss: 3.3238\n",
      "Logging step 12400 at epoch 2.48, train_loss: 3.4608\n",
      "Logging step 12450 at epoch 2.49, train_loss: 3.3472\n",
      "Logging step 12500 at epoch 2.5, train_loss: 3.3808\n",
      "Logging step 12550 at epoch 2.51, train_loss: 3.3964\n",
      "Logging step 12600 at epoch 2.52, train_loss: 3.2762\n",
      "Logging step 12650 at epoch 2.5300000000000002, train_loss: 3.2744\n",
      "Logging step 12700 at epoch 2.54, train_loss: 3.41\n",
      "Logging step 12750 at epoch 2.55, train_loss: 3.2334\n",
      "Logging step 12800 at epoch 2.56, train_loss: 3.2092\n",
      "Logging step 12850 at epoch 2.57, train_loss: 3.3796\n",
      "Logging step 12900 at epoch 2.58, train_loss: 3.4352\n",
      "Logging step 12950 at epoch 2.59, train_loss: 3.4064\n",
      "Logging step 13000 at epoch 2.6, train_loss: 3.2841\n",
      "Logging step 13050 at epoch 2.61, train_loss: 3.1996\n",
      "Logging step 13100 at epoch 2.62, train_loss: 3.4941\n",
      "Logging step 13150 at epoch 2.63, train_loss: 3.4494\n",
      "Logging step 13200 at epoch 2.64, train_loss: 3.2688\n",
      "Logging step 13250 at epoch 2.65, train_loss: 3.3725\n",
      "Logging step 13300 at epoch 2.66, train_loss: 3.3347\n",
      "Logging step 13350 at epoch 2.67, train_loss: 3.3341\n",
      "Logging step 13400 at epoch 2.68, train_loss: 3.4245\n",
      "Logging step 13450 at epoch 2.69, train_loss: 3.42\n",
      "Logging step 13500 at epoch 2.7, train_loss: 3.2874\n",
      "Logging step 13550 at epoch 2.71, train_loss: 3.293\n",
      "Logging step 13600 at epoch 2.7199999999999998, train_loss: 3.2908\n",
      "Logging step 13650 at epoch 2.73, train_loss: 3.2412\n",
      "Logging step 13700 at epoch 2.74, train_loss: 3.5367\n",
      "Logging step 13750 at epoch 2.75, train_loss: 3.2025\n",
      "Logging step 13800 at epoch 2.76, train_loss: 3.4252\n",
      "Logging step 13850 at epoch 2.77, train_loss: 3.2337\n",
      "Logging step 13900 at epoch 2.7800000000000002, train_loss: 3.2906\n",
      "Logging step 13950 at epoch 2.79, train_loss: 3.3215\n",
      "Logging step 14000 at epoch 2.8, train_loss: 3.4091\n",
      "Logging step 14050 at epoch 2.81, train_loss: 3.3406\n",
      "Logging step 14100 at epoch 2.82, train_loss: 3.3202\n",
      "Logging step 14150 at epoch 2.83, train_loss: 3.2631\n",
      "Logging step 14200 at epoch 2.84, train_loss: 3.3822\n",
      "Logging step 14250 at epoch 2.85, train_loss: 3.2043\n",
      "Logging step 14300 at epoch 2.86, train_loss: 3.2022\n",
      "Logging step 14350 at epoch 2.87, train_loss: 3.3291\n",
      "Logging step 14400 at epoch 2.88, train_loss: 3.3833\n",
      "Logging step 14450 at epoch 2.89, train_loss: 3.4807\n",
      "Logging step 14500 at epoch 2.9, train_loss: 3.3084\n",
      "Logging step 14550 at epoch 2.91, train_loss: 3.4531\n",
      "Logging step 14600 at epoch 2.92, train_loss: 3.3244\n",
      "Logging step 14650 at epoch 2.93, train_loss: 3.5245\n",
      "Logging step 14700 at epoch 2.94, train_loss: 3.6776\n",
      "Logging step 14750 at epoch 2.95, train_loss: 3.2815\n",
      "Logging step 14800 at epoch 2.96, train_loss: 3.4037\n",
      "Logging step 14850 at epoch 2.9699999999999998, train_loss: 3.4058\n",
      "Logging step 14900 at epoch 2.98, train_loss: 3.2837\n",
      "Logging step 14950 at epoch 2.99, train_loss: 3.103\n",
      "Logging step 15000 at epoch 3.0, train_loss: 3.3189\n",
      "Logging step 15050 at epoch 3.01, train_loss: 3.2135\n",
      "Logging step 15100 at epoch 3.02, train_loss: 3.3339\n",
      "Logging step 15150 at epoch 3.03, train_loss: 3.4443\n",
      "Logging step 15200 at epoch 3.04, train_loss: 3.4715\n",
      "Logging step 15250 at epoch 3.05, train_loss: 3.2714\n",
      "Logging step 15300 at epoch 3.06, train_loss: 3.3373\n",
      "Logging step 15350 at epoch 3.07, train_loss: 3.4579\n",
      "Logging step 15400 at epoch 3.08, train_loss: 3.2034\n",
      "Logging step 15450 at epoch 3.09, train_loss: 3.3396\n",
      "Logging step 15500 at epoch 3.1, train_loss: 3.2112\n",
      "Logging step 15550 at epoch 3.11, train_loss: 3.6365\n",
      "Logging step 15600 at epoch 3.12, train_loss: 3.2851\n",
      "Logging step 15650 at epoch 3.13, train_loss: 3.2126\n",
      "Logging step 15700 at epoch 3.14, train_loss: 3.3811\n",
      "Logging step 15750 at epoch 3.15, train_loss: 3.4071\n",
      "Logging step 15800 at epoch 3.16, train_loss: 3.5654\n",
      "Logging step 15850 at epoch 3.17, train_loss: 3.4444\n",
      "Logging step 15900 at epoch 3.18, train_loss: 3.1605\n",
      "Logging step 15950 at epoch 3.19, train_loss: 3.2946\n",
      "Logging step 16000 at epoch 3.2, train_loss: 3.4094\n",
      "Logging step 16050 at epoch 3.21, train_loss: 3.0947\n",
      "Logging step 16100 at epoch 3.22, train_loss: 3.3566\n",
      "Logging step 16150 at epoch 3.23, train_loss: 3.3136\n",
      "Logging step 16200 at epoch 3.24, train_loss: 3.4155\n",
      "Logging step 16250 at epoch 3.25, train_loss: 3.3564\n",
      "Logging step 16300 at epoch 3.26, train_loss: 3.5322\n",
      "Logging step 16350 at epoch 3.27, train_loss: 3.2084\n",
      "Logging step 16400 at epoch 3.2800000000000002, train_loss: 3.2561\n",
      "Logging step 16450 at epoch 3.29, train_loss: 3.4558\n",
      "Logging step 16500 at epoch 3.3, train_loss: 3.2763\n",
      "Logging step 16550 at epoch 3.31, train_loss: 3.33\n",
      "Logging step 16600 at epoch 3.32, train_loss: 3.3609\n",
      "Logging step 16650 at epoch 3.33, train_loss: 3.2986\n",
      "Logging step 16700 at epoch 3.34, train_loss: 3.4087\n",
      "Logging step 16750 at epoch 3.35, train_loss: 3.3435\n",
      "Logging step 16800 at epoch 3.36, train_loss: 3.3292\n",
      "Logging step 16850 at epoch 3.37, train_loss: 3.309\n",
      "Logging step 16900 at epoch 3.38, train_loss: 3.3055\n",
      "Logging step 16950 at epoch 3.39, train_loss: 3.3831\n",
      "Logging step 17000 at epoch 3.4, train_loss: 3.3412\n",
      "Logging step 17050 at epoch 3.41, train_loss: 3.3954\n",
      "Logging step 17100 at epoch 3.42, train_loss: 3.4323\n",
      "Logging step 17150 at epoch 3.43, train_loss: 3.3522\n",
      "Logging step 17200 at epoch 3.44, train_loss: 3.316\n",
      "Logging step 17250 at epoch 3.45, train_loss: 3.4337\n",
      "Logging step 17300 at epoch 3.46, train_loss: 3.3529\n",
      "Logging step 17350 at epoch 3.4699999999999998, train_loss: 3.4406\n",
      "Logging step 17400 at epoch 3.48, train_loss: 3.3878\n",
      "Logging step 17450 at epoch 3.49, train_loss: 3.2553\n",
      "Logging step 17500 at epoch 3.5, train_loss: 3.3075\n",
      "Logging step 17550 at epoch 3.51, train_loss: 3.408\n",
      "Logging step 17600 at epoch 3.52, train_loss: 3.2949\n",
      "Logging step 17650 at epoch 3.5300000000000002, train_loss: 3.3934\n",
      "Logging step 17700 at epoch 3.54, train_loss: 3.4163\n",
      "Logging step 17750 at epoch 3.55, train_loss: 3.282\n",
      "Logging step 17800 at epoch 3.56, train_loss: 3.2157\n",
      "Logging step 17850 at epoch 3.57, train_loss: 3.4313\n",
      "Logging step 17900 at epoch 3.58, train_loss: 3.431\n",
      "Logging step 17950 at epoch 3.59, train_loss: 3.3802\n",
      "Logging step 18000 at epoch 3.6, train_loss: 3.2442\n",
      "Logging step 18050 at epoch 3.61, train_loss: 3.3995\n",
      "Logging step 18100 at epoch 3.62, train_loss: 3.3442\n",
      "Logging step 18150 at epoch 3.63, train_loss: 3.395\n",
      "Logging step 18200 at epoch 3.64, train_loss: 3.2101\n",
      "Logging step 18250 at epoch 3.65, train_loss: 3.2424\n",
      "Logging step 18300 at epoch 3.66, train_loss: 3.5669\n",
      "Logging step 18350 at epoch 3.67, train_loss: 3.396\n",
      "Logging step 18400 at epoch 3.68, train_loss: 3.3594\n",
      "Logging step 18450 at epoch 3.69, train_loss: 3.3513\n",
      "Logging step 18500 at epoch 3.7, train_loss: 3.2546\n",
      "Logging step 18550 at epoch 3.71, train_loss: 3.3509\n",
      "Logging step 18600 at epoch 3.7199999999999998, train_loss: 3.2467\n",
      "Logging step 18650 at epoch 3.73, train_loss: 3.3259\n",
      "Logging step 18700 at epoch 3.74, train_loss: 3.3051\n",
      "Logging step 18750 at epoch 3.75, train_loss: 3.2383\n",
      "Logging step 18800 at epoch 3.76, train_loss: 3.2607\n",
      "Logging step 18850 at epoch 3.77, train_loss: 3.3272\n",
      "Logging step 18900 at epoch 3.7800000000000002, train_loss: 3.3904\n",
      "Logging step 18950 at epoch 3.79, train_loss: 3.4156\n",
      "Logging step 19000 at epoch 3.8, train_loss: 3.3855\n",
      "Logging step 19050 at epoch 3.81, train_loss: 3.1806\n",
      "Logging step 19100 at epoch 3.82, train_loss: 3.165\n",
      "Logging step 19150 at epoch 3.83, train_loss: 3.2749\n",
      "Logging step 19200 at epoch 3.84, train_loss: 3.2913\n",
      "Logging step 19250 at epoch 3.85, train_loss: 3.2035\n",
      "Logging step 19300 at epoch 3.86, train_loss: 3.2187\n",
      "Logging step 19350 at epoch 3.87, train_loss: 3.3783\n",
      "Logging step 19400 at epoch 3.88, train_loss: 3.4731\n",
      "Logging step 19450 at epoch 3.89, train_loss: 3.2182\n",
      "Logging step 19500 at epoch 3.9, train_loss: 3.3523\n",
      "Logging step 19550 at epoch 3.91, train_loss: 3.3154\n",
      "Logging step 19600 at epoch 3.92, train_loss: 3.3965\n",
      "Logging step 19650 at epoch 3.93, train_loss: 3.3051\n",
      "Logging step 19700 at epoch 3.94, train_loss: 3.2859\n",
      "Logging step 19750 at epoch 3.95, train_loss: 3.3819\n",
      "Logging step 19800 at epoch 3.96, train_loss: 3.2568\n",
      "Logging step 19850 at epoch 3.9699999999999998, train_loss: 3.2529\n",
      "Logging step 19900 at epoch 3.98, train_loss: 3.225\n",
      "Logging step 19950 at epoch 3.99, train_loss: 3.2978\n",
      "Logging step 20000 at epoch 4.0, train_loss: 3.2229\n",
      "Logging step 20050 at epoch 4.01, train_loss: 3.3127\n",
      "Logging step 20100 at epoch 4.02, train_loss: 3.3292\n",
      "Logging step 20150 at epoch 4.03, train_loss: 3.2093\n",
      "Logging step 20200 at epoch 4.04, train_loss: 3.4286\n",
      "Logging step 20250 at epoch 4.05, train_loss: 3.2948\n",
      "Logging step 20300 at epoch 4.06, train_loss: 3.3513\n",
      "Logging step 20350 at epoch 4.07, train_loss: 3.3839\n",
      "Logging step 20400 at epoch 4.08, train_loss: 3.2615\n",
      "Logging step 20450 at epoch 4.09, train_loss: 3.2978\n",
      "Logging step 20500 at epoch 4.1, train_loss: 3.3476\n",
      "Logging step 20550 at epoch 4.11, train_loss: 3.2787\n",
      "Logging step 20600 at epoch 4.12, train_loss: 3.1842\n",
      "Logging step 20650 at epoch 4.13, train_loss: 3.144\n",
      "Logging step 20700 at epoch 4.14, train_loss: 3.2003\n",
      "Logging step 20750 at epoch 4.15, train_loss: 3.304\n",
      "Logging step 20800 at epoch 4.16, train_loss: 3.3704\n",
      "Logging step 20850 at epoch 4.17, train_loss: 3.2842\n",
      "Logging step 20900 at epoch 4.18, train_loss: 3.4844\n",
      "Logging step 20950 at epoch 4.19, train_loss: 3.2914\n",
      "Logging step 21000 at epoch 4.2, train_loss: 3.1749\n",
      "Logging step 21050 at epoch 4.21, train_loss: 3.3692\n",
      "Logging step 21100 at epoch 4.22, train_loss: 3.3956\n",
      "Logging step 21150 at epoch 4.23, train_loss: 3.2288\n",
      "Logging step 21200 at epoch 4.24, train_loss: 3.3448\n",
      "Logging step 21250 at epoch 4.25, train_loss: 3.3744\n",
      "Logging step 21300 at epoch 4.26, train_loss: 3.3476\n",
      "Logging step 21350 at epoch 4.27, train_loss: 3.3626\n",
      "Logging step 21400 at epoch 4.28, train_loss: 3.4139\n",
      "Logging step 21450 at epoch 4.29, train_loss: 3.2391\n",
      "Logging step 21500 at epoch 4.3, train_loss: 3.2877\n",
      "Logging step 21550 at epoch 4.31, train_loss: 3.4324\n",
      "Logging step 21600 at epoch 4.32, train_loss: 3.3469\n",
      "Logging step 21650 at epoch 4.33, train_loss: 3.4553\n",
      "Logging step 21700 at epoch 4.34, train_loss: 3.1869\n",
      "Logging step 21750 at epoch 4.35, train_loss: 3.2859\n",
      "Logging step 21800 at epoch 4.36, train_loss: 3.445\n",
      "Logging step 21850 at epoch 4.37, train_loss: 3.2263\n",
      "Logging step 21900 at epoch 4.38, train_loss: 3.3486\n",
      "Logging step 21950 at epoch 4.39, train_loss: 3.2925\n",
      "Logging step 22000 at epoch 4.4, train_loss: 3.3088\n",
      "Logging step 22050 at epoch 4.41, train_loss: 3.3967\n",
      "Logging step 22100 at epoch 4.42, train_loss: 3.3265\n",
      "Logging step 22150 at epoch 4.43, train_loss: 3.286\n",
      "Logging step 22200 at epoch 4.44, train_loss: 3.3873\n",
      "Logging step 22250 at epoch 4.45, train_loss: 3.3226\n",
      "Logging step 22300 at epoch 4.46, train_loss: 3.2397\n",
      "Logging step 22350 at epoch 4.47, train_loss: 3.3583\n",
      "Logging step 22400 at epoch 4.48, train_loss: 3.3218\n",
      "Logging step 22450 at epoch 4.49, train_loss: 3.2761\n",
      "Logging step 22500 at epoch 4.5, train_loss: 3.1837\n",
      "Logging step 22550 at epoch 4.51, train_loss: 3.3529\n",
      "Logging step 22600 at epoch 4.52, train_loss: 3.5033\n",
      "Logging step 22650 at epoch 4.53, train_loss: 3.3174\n",
      "Logging step 22700 at epoch 4.54, train_loss: 3.1785\n",
      "Logging step 22750 at epoch 4.55, train_loss: 3.3311\n",
      "Logging step 22800 at epoch 4.5600000000000005, train_loss: 3.4512\n",
      "Logging step 22850 at epoch 4.57, train_loss: 3.4012\n",
      "Logging step 22900 at epoch 4.58, train_loss: 3.2545\n",
      "Logging step 22950 at epoch 4.59, train_loss: 3.3822\n",
      "Logging step 23000 at epoch 4.6, train_loss: 3.3407\n",
      "Logging step 23050 at epoch 4.61, train_loss: 3.2783\n",
      "Logging step 23100 at epoch 4.62, train_loss: 3.2325\n",
      "Logging step 23150 at epoch 4.63, train_loss: 3.2062\n",
      "Logging step 23200 at epoch 4.64, train_loss: 3.3755\n",
      "Logging step 23250 at epoch 4.65, train_loss: 3.2673\n",
      "Logging step 23300 at epoch 4.66, train_loss: 3.2056\n",
      "Logging step 23350 at epoch 4.67, train_loss: 3.383\n",
      "Logging step 23400 at epoch 4.68, train_loss: 3.5343\n",
      "Logging step 23450 at epoch 4.6899999999999995, train_loss: 3.4155\n",
      "Logging step 23500 at epoch 4.7, train_loss: 3.2888\n",
      "Logging step 23550 at epoch 4.71, train_loss: 3.446\n",
      "Logging step 23600 at epoch 4.72, train_loss: 3.3108\n",
      "Logging step 23650 at epoch 4.73, train_loss: 3.3594\n",
      "Logging step 23700 at epoch 4.74, train_loss: 3.419\n",
      "Logging step 23750 at epoch 4.75, train_loss: 3.2492\n",
      "Logging step 23800 at epoch 4.76, train_loss: 3.3171\n",
      "Logging step 23850 at epoch 4.77, train_loss: 3.3184\n",
      "Logging step 23900 at epoch 4.78, train_loss: 3.3525\n",
      "Logging step 23950 at epoch 4.79, train_loss: 3.2486\n",
      "Logging step 24000 at epoch 4.8, train_loss: 3.3213\n",
      "Logging step 24050 at epoch 4.8100000000000005, train_loss: 3.6006\n",
      "Logging step 24100 at epoch 4.82, train_loss: 3.4275\n",
      "Logging step 24150 at epoch 4.83, train_loss: 3.2707\n",
      "Logging step 24200 at epoch 4.84, train_loss: 3.2822\n",
      "Logging step 24250 at epoch 4.85, train_loss: 3.2686\n",
      "Logging step 24300 at epoch 4.86, train_loss: 3.3532\n",
      "Logging step 24350 at epoch 4.87, train_loss: 3.3835\n",
      "Logging step 24400 at epoch 4.88, train_loss: 3.346\n",
      "Logging step 24450 at epoch 4.89, train_loss: 3.2142\n",
      "Logging step 24500 at epoch 4.9, train_loss: 3.4988\n",
      "Logging step 24550 at epoch 4.91, train_loss: 3.3432\n",
      "Logging step 24600 at epoch 4.92, train_loss: 3.1877\n",
      "Logging step 24650 at epoch 4.93, train_loss: 3.3825\n",
      "Logging step 24700 at epoch 4.9399999999999995, train_loss: 3.4131\n",
      "Logging step 24750 at epoch 4.95, train_loss: 3.2695\n",
      "Logging step 24800 at epoch 4.96, train_loss: 3.2463\n",
      "Logging step 24850 at epoch 4.97, train_loss: 3.3813\n",
      "Logging step 24900 at epoch 4.98, train_loss: 3.476\n",
      "Logging step 24950 at epoch 4.99, train_loss: 3.3507\n",
      "Logging step 25000 at epoch 5.0, train_loss: 3.3036\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f690212-8828-4709-895a-c236646e128b",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f6c510-5378-4701-aaa3-4c5f777e8718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-fs/Llama-3-12B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Llama3_LoRA_1/tokenizer_config.json',\n",
       " 'Llama3_LoRA_1/special_tokens_map.json',\n",
       " 'Llama3_LoRA_1/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_id = \"Llama3_LoRA_1\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635b12b-7fda-4e1c-9177-db1c4cd76cb6",
   "metadata": {},
   "source": [
    "# <center><font color=red>Load LoRA-model to do one-chat-test</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87425052-e3f9-4965-bc4e-b055c07325d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d24f085f034f3cb4099e8dadf76235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "'''================================================'''\n",
    "peft_model_id = \"Llama3_LoRA_1\"\n",
    "'''================================================'''\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "if config.base_model_name_or_path:\n",
    "    ;\n",
    "else:\n",
    "    '''========================================================'''\n",
    "    config.base_model_name_or_path = \"/root/autodl-fs/Llama-3-12B\"\n",
    "    '''========================================================'''\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, torch_dtype=torch.bfloat16,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "    \n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7129891e-9e87-4ee4-aed2-9e5121b2d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pad_token_id to eos_token_id if not already set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Ensure the model uses this pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a78287-f6bb-450a-97e6-9d4f0168b2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 1,\n",
       " 'conv': 'Hey there hows it going! You like catch me if you can as much as i do?',\n",
       " 'response': 'Opps I meant means girls!'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset_file = \"pairs/test_pairs.json\"\n",
    "df_test = pd.read_json(dataset_file)\n",
    "\n",
    "'''---------------------'''\n",
    "df_test = df_test\n",
    "'''---------------------'''\n",
    "\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "ds_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58e5e1ad-1eb2-4508-8e8e-158a1b5fd197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: The main character, named Chrissie, was at the party. She went skinny dipping in the ocean and was pulled under the water. The next day, her remains washed ashore. \n",
      "response:\n",
      " Her best friend Amy started to wonder what happened.\n",
      "\n",
      "When she found out that Chrissie's body had been discovered onshore, it made things even more complicated.canfriends\n",
      "\n",
      "It is determined by the police officers that there were no witnesses who saw anything related to Chrissie or any other person\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "prompt = ds_test[randrange(len(ds_test))]['conv']\n",
    "print(\"prompt:\",prompt)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model_inputs = tokenizer([text], padding='max_length', return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=60,\n",
    "    eos_token_id=tokenizer.encode('<|eot_id|>')[0],\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    do_sample = True,\n",
    "    temperature=0.9,\n",
    "    top_k=100,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"response:\\n\",response)\n",
    "\n",
    "# generated_ids = model.generate(\n",
    "#     model_inputs.input_ids,\n",
    "#     max_new_tokens=512,\n",
    "#     pad_token_id = model.config.pad_token_id,\n",
    "#     eos_token_id=tokenizer.encode('<|eot_id|>')[0]\n",
    "# )\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6069362-0513-4a2e-abf1-c76e94ce8b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
